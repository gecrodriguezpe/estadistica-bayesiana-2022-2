---
title: "Regresión Poisson"
author: 
- Juan Sosa PhD
- Webpage https://sites.google.com/view/juansosa/ 
- YouTube https://www.youtube.com/c/JuanSosa1702 
- GitHub  https://github.com/jstats1702 
- Rpubs   https://rpubs.com/jstats1702
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Modelo

A continuación se presenta la formulación básica de un modelo de Regresión Poisson como un **modelo lineal generalizado** (GLM, *generalized linear model*):

**Distribución muestral**:
$$
y_i\mid\theta_i\stackrel{\text {iid}}{\sim}\textsf{Poisson}(\theta_i)\,,\qquad i = 1,\ldots,n
$$
donde
$$
\log(\theta_i) = \boldsymbol{\beta}^{\textsf{T}}\boldsymbol{x}_i = \sum_{j=1}^k\beta_j\, x_{i,j}
$$
con $\boldsymbol{\beta}=(\beta_1,\ldots,\beta_k)$ y $\boldsymbol{x} = (x_{i,1},\ldots,x_{i,k})$. 

Observe que la **función de enlace** (*link function*) en este modelo lineal generalizado es la función $\log$.

**Distribuciones previas conjugadas** para modelos lineales generalizados **no existen** (a excepción del modelo de regresión Normal).

**Distribución previa**:
$$
\boldsymbol{\beta}\sim\textsf{N}(\boldsymbol{\beta}_0,\Sigma_0)
$$

Los **parámetros** del modelo son $(\beta_1,\ldots,\beta_k)$.

Los **hiperparámetros** del modelo son $(\boldsymbol{\beta}_0,\Sigma_0)$.

# Distibución conjunta posterior

$$
\begin{align*}
p(\boldsymbol{\beta}\mid\boldsymbol{y}) &\propto p(\boldsymbol{y}\mid\boldsymbol{\beta})\,p(\boldsymbol{\beta}) \\
&= \prod_{i=1}^n \textsf{Poisson}(y_i\mid\theta_i) \times\textsf{N}(\boldsymbol{\beta}_0,\Sigma_0) \\
&= \prod_{i=1}^n \frac{e^{-\theta_i}\,\theta_i^{y_i}}{y_i!} \times \exp\left\{ -\tfrac{1}{2}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^{\textsf{T}}\Sigma_0^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_0) \right\} \\
&\propto \prod_{i=1}^n e^{-\theta_i}\,\theta_i^{y_i} \times \exp\left\{ -\tfrac{1}{2} \left[ \boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta}_0 \right]   \right\}
\end{align*}
$$
con $\log(\theta_i) = \boldsymbol{\beta}^{\textsf{T}}\boldsymbol{x}_i$. 

Por lo tanto, en **escala logaritmica**, se tiene que
$$
\log p(\boldsymbol{\beta}\mid\boldsymbol{y}) \propto \sum_{i=1}^n(y_i\log(\theta_i) - \theta_i) -\tfrac{1}{2} \left[ \boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta}_0 \right] 
$$

Observe que $p(\boldsymbol{\beta}\mid\boldsymbol{y})$ no corresponde a ninguna familia paramétrica de distribuciones conocida.

# Algorithmo de Metropolis-Hastings

El algoritmo Metropolis-Hastings es un método genérico de aproximación de **cualquier distribución posterior**.

El problema radica cuando $p(\boldsymbol{\theta}\mid \boldsymbol{y})$ no tiene una distribución estándar conocida de la cual sea posible simular fácilmente.

## Metropolis

**Algoritmo:**

Dado un estado actual $\theta^{(b)}$ del parámetro $\theta$, se genera el siguiente estado $\theta^{(b+1)}$ como sigue: 

1. Simular $\theta^*\sim J(\theta\mid\theta^{(b)})$ donde $J$ es una distribución **simétrica**: $J(\theta_1\mid\theta_2) = J(\theta_2\mid\theta_1)$.
Esta distribución se denomina **distribución de propuestas** (*proposal distribution*). Típicamente,
$$
J(\theta\mid\theta^{(b)}) = \textsf{N}(\theta\mid\theta^{(b)},\delta^2)\qquad\text{o}\qquad J(\theta\mid\theta^{(b)}) = \textsf{U}(\theta\mid\theta^{(b)} - \delta, \theta^{(b)} + \delta)
$$
donde $\delta$, conocido como **parámetro de ajuste**, se escoge de forma que el algoritmo funcione eficientemente.
2. Calcular la tasa de aceptación:
$$
r = \frac{p(\theta^*\mid y)}{p(\theta^{(b)}\mid y)}\,.
$$
Para brindar estabilidad numérica en el procesamiento, esta tasa usualmente se calcula primero en escala logarítmica:
$$
r = \exp\left( \log p(\theta^*\mid y) - \log p(\theta^{(b)}\mid y) \right)\,.
$$
3. Establecer:
$$
\theta^{(b+1)} =
\left\{
  \begin{array}{ll}
    \theta^*\,     , & \hbox{con probabilidad $\min(1,r)$;} \\
    \theta^{(b)}\, , & \hbox{con probabilidad $1-\min(1,r)$.}
  \end{array}
\right.
$$

## Observaciones

- Este algoritmo produce una secuencia de valores $\theta^{(1)},\ldots,\theta^{(B)}$, donde $\theta^{(b+1)}$ depende de los valores previos únicamente a través de $\theta^{(b)}$, y por lo tanto la secuencia es una **Cadena de Markov**.
- La **correlación de la cadena** depende del valor de $\delta$ en la distribución de propuestas. 
- Es una práctica común **elegir un valor** de $\delta$ tal que la **tasa de aceptación** esté entre 30\% y 50\%. Existen **algoritmos adaptativos** para elegir automáticamente el valor de $\delta$.

## Metropolis-Hastings

El algoritmo Metrópolis-Hastings **generaliza los algoritmos de Gibbs** y **Metropolis** al permitir una distribución de propuesta arbitraria:

**Algoritmo:**

Dado un estado actual $\theta^{(b)}$ del parámetro $\theta$, se genera el siguiente estado $\theta^{(b+1)}$ como sigue: 

1. Simular $\theta^*\sim J(\theta\mid\theta^{(b)})$ donde $J$ es una **distribución de propuestas arbitraria**.

2. Calcular la tasa de aceptación:
$$
r = \frac{p(\theta^*\mid y)}{p(\theta^{(b)}\mid y)}\,\frac{J(\theta^{(b)}\mid\theta^*)}{J(\theta^*\mid \theta^{(b)})}\,.
$$
3. Establecer:
$$
\theta^{(b+1)} =
\left\{
  \begin{array}{ll}
    \theta^*\,     , & \hbox{con probabilidad $\min(1,r)$;} \\
    \theta^{(b)}\, , & \hbox{con probabilidad $1-\min(1,r)$.}
  \end{array}
\right.
$$

## Observaciones

- Cada paso del **algoritmo de Gibbs** se puede ver como la generación de una propuesta de una distribución condicional completa y luego aceptarla con probabilidad 1.
- El algoritmo de Gibbs y el algoritmo de Metropolis son **casos particulares** del algoritmo de Metropolis-Hastings.


# Ejemplo: Modelo Normal

Algoritmo de Metropolis en un modelo Normal con varianza conocida.

**Distribución muestral:**
$$
y_i\mid\theta \stackrel{\text {iid}}{\sim}\textsf{Normal}(\theta, 1)\,,\qquad i = 1,\ldots,n
$$

**Distribución previa:**
$$
\theta \sim \textsf{Normal}(\mu, \tau^2)
$$


```{r}
# simular datos
n  <- 5
s2 <- 1 
set.seed(1)
y <- round(rnorm(n = n, mean = 10, sd = 1), 2)
# previa
t2 <- 10 
mu <- 5
# Metropolis
theta <- 0      # valor inicial
delta <- 1.75   # parámetro de ajuste
S     <- 10000  # numero de muestras
THETA <- NULL   # almacenamiento
# cadena
set.seed(1)
for (s in 1:S) {
  # 1. propuesta
  theta.star <- rnorm(1,theta,sqrt(delta))
  # 2. tasa de aceptación
  log.r <- (sum(dnorm(y, theta.star, sqrt(s2), log = T)) + dnorm(theta.star, mu, sqrt(t2), log = T)) - (sum(dnorm(y, theta, sqrt(s2), log = T)) + dnorm(theta, mu, sqrt(t2), log = T)) 
  # 3. actualizar
  if (runif(1) < exp(log.r)) 
    theta <- theta.star 
  # 4. almacenar
  THETA <- c(THETA, theta)
}
```


```{r, fig.height=5, fig.width=10, echo = F, fig.align='center'}
# grafico
par(mfrow = c(1,2), mar = c(3,3,1,1), mgp = c(1.75,0.75,0))
# cadena
plot(x = 1:S, y = THETA, type = "l", xlab = "Iteración", ylab = expression(theta))
# histograma metropolis
# se omiten las primeras 100 observaciones (periodo de calentamiento)
hist(x = THETA[-(1:100)], prob = T, main = "", xlab = expression(theta), ylab = "Densidad", col = "gray", border = "gray")
# posterior analítica
th <- seq(min(THETA), max(THETA), length = 1000)
mu.n <- (mu/t2 + mean(y)*n/s2)/(1/t2 + n/s2) 
t2.n <- 1/(1/t2 + n/s2)
lines(x = th, y = dnorm(th, mu.n, sqrt(t2.n)), col = 2, lty = 1, lwd = 1)
```


```{r}
ACR    <- NULL  # tasa de aceptaciones
ACF    <- NULL  # autocorrelaciones
THETAA <- NULL  # muestras
for(delta2 in 2^c(-5,-1,1,5,7) ) {
  # parametros iniciales
  THETA <- NULL
  S     <- 10000
  theta <- 0
  acs   <- 0  # tasa de aceptacion
  # cadena
  set.seed(1)
  for(s in 1:S) {
    # 1. propuesta
    theta.star<-rnorm(1,theta,sqrt(delta2))
    # 2. tasa de aceptacion
    log.r <- (sum(dnorm(y, theta.star, sqrt(s2), log = T)) + dnorm(theta.star, mu, sqrt(t2), log = T)) - (sum(dnorm(y, theta, sqrt(s2), log = T)) + dnorm(theta, mu, sqrt(t2), log = T)) 
    # 3. actualizar
    if(runif(1) < exp(log.r)) { 
      theta <- theta.star 
      acs   <- acs + 1 
    }
    # 4. almacenar
    THETA <- c(THETA, theta) 
  }
  # fin MCMC
  # almacenar valores de todos los casos (delta2)
  ACR    <- c(ACR, acs/s) 
  ACF    <- c(ACF, acf(THETA, plot = F)$acf[2])
  THETAA <- cbind(THETAA, THETA)
}
```


```{r, fig.height=4, fig.width=12, fig.align='center', echo = F}
# gráficos
par(mfrow = c(1,3), mar = c(2.75,2.75,.5,.5), mgp = c(1.7,0.75,0))
laby <- c(expression(theta),"","","","")
for (k in c(1,3,5)) {
  plot(x = THETAA[1:500,k], type = "l", xlab = "Iteración", ylab = laby[k], ylim = range(THETAA))
  abline(h = mu.n, lty = 2)
}
```


```{r}
# tasas de aceptacion
round(ACR, 3)
# autocorrelaciones
round(ACF ,3)
# tamaños efectivos de muestra
round(coda::effectiveSize(THETAA), 3)
```


# Ejemplo: Regresión Poisson

Actividades de reproducción de gorriones en función de la edad.

Caracterizar el número de crías en función de la edad.

***Arcese, P., Smith, J. N., Hochachka, W. M., Rogers, C. M., & Ludwig, D. (1992). Stability, regulation, and the determination of abundance in an insular song sparrow population. Ecology, 73(3), 805-822.***

```{r, eval = TRUE, echo=FALSE, out.width="50%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("sparrow.jpg")
```

- $y_i\,\,\,\,$: número de crías, para $i=1,\ldots,n$.
- $x_{i,j}\,$: predictor $j$ observado en el individuo $i$, para $i=1,\ldots,n$ y $j=1,\ldots,k$.

```{r}
#-------------------------------------------------------------------------------
# Descripcion: 
# Actividades de reproduccion de gorriones en funcion de la edad (Arcese et al, 1992).
# n = 52 gorriones hembras.
# "age"     : edad.
# "fledged" : numero de crias.
#-------------------------------------------------------------------------------

############
### data ###
############
spfage <- structure(c(3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 4, 7, 2, 2, 1, 
                      1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 4, 1, 2, 5, 1, 2, 
                      1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 
                      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                      1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 
                      2, 2, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 
                      4, 4, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1, 9, 9, 1, 1, 
                      1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 25, 25, 16, 16, 
                      16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 25, 16, 16, 16, 16, 
                      25, 25, 25, 25, 9, 9, 9, 9, 9, 9, 9, 36, 1, 1), 
                    .Dim = c(52L, 4L), 
                    .Dimnames = list(NULL, c("fledged", "intercept", "age", "age2")))
spfage <- as.data.frame(spfage)
# diseño
spf  <- spfage$fledged  # y  = variable dependiente (respuesta)
age  <- spfage$age      # x1 = variable independiente 1
age2 <- age^2           # x2 = variable independiente 2
```


```{r, fig.width=10, fig.height=5}
############################
### analisis descriptivo ###
############################
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(spf~as.factor(age), range=0, xlab="Edad (años)", ylab="No. Crias", col="gray", border="lightgray")
```


```{r}
# GLM (frecuentista)
summary(glm(spf~age+age2,family="poisson"))
```

En muchos problemas, la varianza posterior es una elección eficiente para la 
distribución de propuestas. Aunque no se conoce la varianza posterior antes de 
ejecutar el algoritmo de Metrópolis, basta con utilizar una aproximación.
Si esto da como resultado una tasa de aceptación demasiado alta o demasiado baja, 
siempre es posible ajustar la variabilidad de la propuesta en consecuencia.


```{r}
# datos 
y <- spf                                # variable respuesta
X <- cbind(rep(1,length(y)),age,age^2)  # matriz de diseño
n <- length(y)                          # tamaño de la muestrsa
p <- dim(X)[2]                          # numero de predictores
# previa
pmn.beta <- rep(0,  p)  # beta0 = 0
psd.beta <- rep(10, p)  # Sigma0 = 10*I
# log: funcion de enlace 
# y + 1/2 evitar problemas en la frontera con 0
var.prop <- var(log(y + 1)) * solve(t(X)%*%X) # matriz de varianza propuesta
beta <- rep(0,p) # valor inicial beta
######## algoritmo de metropolis
S    <- 50000
BETA <- matrix(data = NA, nrow = S, ncol = p)
ac   <- 0
ncat <- floor(S/10)
######## cadena
set.seed(1)
for(s in 1:S) {
  # 1. propuesta
  beta.p <- c(mvtnorm::rmvnorm(n = 1, mean = beta, sigma = var.prop))
  # 2. tasa de aceptacion
  lhr <- sum(dpois(x = y, lambda = exp(X%*%beta.p), log = T)) - sum(dpois(x = y, lambda = exp(X%*%beta),log = T)) + sum(dnorm(x = beta.p, mean = pmn.beta, sd = psd.beta, log = T)) - sum(dnorm(x = beta, mean = pmn.beta, sd = psd.beta, log = T))
  # 3. actualizar
  if (runif(1) < exp(lhr)) { 
    beta <- beta.p 
    ac   <- ac + 1 
  }
  # 4. almacenar
  BETA[s,] <- beta
  # 5. Progreso
  if (s%%ncat == 0) cat(100*round(s/S, 1), "% completed ... \n", sep = "" )
}
######### fin mcmc
```


```{r}
# tasa de aceptacion
round(100*ac/S, 1)
# diagnosticos
round(apply(X = BETA, MARGIN = 2, FUN = coda::effectiveSize), 1)
```


```{r, fig.height=4, fig.width=12, echo = F, fig.align='center'}
# grafico diagnostico
blabs <- c(expression(beta[1]),expression(beta[2]),expression(beta[3]))  # etiquetas
thin  <- seq(from = 10, to = S, by = 10)  # muestreo sistematico 
par(mfrow = c(1,3), mar = c(2.75,2.75,.5,.5), mgp = c(1.7,.7,0))
j <- 3
plot(x = thin, y = BETA[thin,j], type = "l", xlab = "Iteración", ylab = blabs[j])
acf (x = BETA[thin,j], xlab = "lag/10")
acf (x = BETA[,j], xlab = "lag")
```

```{r, fig.height=4, fig.width=12, echo = F, fig.align='center'}
par(mfrow=c(1,3), mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
plot(density(BETA[,2]), type="l", main="",
     xlab=expression(beta[2]),
     ylab=expression(paste(italic("p("),beta[2],"|",italic("y)"),sep="") ) ,
     lwd=1,lty=1,col="black")
plot(density(BETA[,3]), type="l", main="",
     xlab=expression(beta[3]),
     ylab=expression(paste(italic("p("),beta[3],"|",italic("y)"),sep="") ),
     lwd=1,col="black",lty=1)
Xs       <- cbind(rep(1,6),1:6,(1:6)^2) 
eXB.post <- exp(t(Xs%*%t(BETA )) )
qE       <- apply( eXB.post,2,quantile,probs=c(.025,.5,.975))
plot(c(1,6),range(c(0,qE)),type="n",xlab="Edad (años)", ylab="No. Crias")
lines(x = qE[1,], col = 4, lwd = 1 )
lines(x = qE[2,], col = 1, lwd = 2 )
lines(x = qE[3,], col = 4, lwd = 1 )
```

```{r}
round(quantile(BETA[,2], c(.025, .975)), 3)
round(quantile(BETA[,3], c(.025, .975)), 3)
round(mean(BETA[,2] > 0), 3)
round(mean(BETA[,3] > 0), 3)
```


```{r, eval=F}
########
# JAGS #
########
library(R2jags)
model <- function() {
    # verosimilitud
    for (i in 1:n) {
        y[i] ~ dpois(theta[i])
        log(theta[i]) <- inprod(X[i,], beta) # X[i,1]*beta[1]+X[i,2]*beta[2]+X[i,3]*beta[3]
    }
    # previa
    for (j in 1:p) {
        beta[j] ~ dnorm(beta0, phi0)    
    }
}
# previa
beta0 <- 0
phi0  <- 1/10
# input
model_data <- list(y = y, X = X, n = n, p = p, beta0 = beta0, phi0 = phi0)
# parameters
model_parameters <- c("beta")
# initial values
initial_values <- list(list("beta" = rep(beta0, p)), 
                       list("beta" = rep(beta0, p)),
                       list("beta" = rep(beta0, p)))
# mcmc settings
niter  <- 26000
nburn  <- 1000
nthin  <- 25
nchain <- length(initial_values)
# mcmc
set.seed(123)
fit <- jags(data = model_data, inits = initial_values, 
            parameters.to.save = model_parameters, model.file = model, 
            n.chains = nchain, n.iter = niter, n.thin = nthin, n.burnin = nburn)
print(fit)
# transformar a objecto MCMC               
fit_mcmc <- coda::as.mcmc(fit)
# cadena
dim(fit$BUGSoutput$sims.list$beta)
# plots
mcmcplots::traplot(fit_mcmc)
mcmcplots::denplot(fit_mcmc)
```


# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```



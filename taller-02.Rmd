---
title: "Taller 2"
author: 
- Juan Sosa PhD
- Webpage https://sites.google.com/view/juansosa/ 
- YouTube https://www.youtube.com/c/JuanSosa1702 
- GitHub  https://github.com/jstats1702 
- Rpubs   https://rpubs.com/jstats1702
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Sean $x$, $y$, y $z$ variables aleatorias con función de densidad conjunta (discreta o continua) dada por $p(x,y,z) \propto p(x,z)p(y,z)p(z)$. Muestre que:

a. $p(x\mid y,z)\propto p(x,z)$, i.e., $p(x\mid y,z)$ es función de $x$ y $z$.
b. $p(y\mid x,z)\propto p(y,z)$, i.e., $p(y\mid x,z)$ es función de $y$ y $z$.
b. $x$ y $y$ son condicionalmente independientes dado $z$.
    
2. Sean $A$, $B$, y $C$ proposiciones de falso-verdadero. Suponga que $A$ y $B$ son condicionalmente independientes, dado $C$. Muestre que:

a. $A$   y $B^C$ son condicionalmente independientes, dado $C$.	
b. $A^C$ y $B^C$ son condicionalmente independientes, dado $C$.

2. Sea $y\mid x\sim\textsf{Poi}(x)$ y $x\sim\textsf{Exp}(\lambda)$.
    
a. Muestre que la distribución marginal de $y$ es:
$$
p(y) = \frac{\lambda}{(\lambda+1)^{y+1}}\,,\qquad y = 0,1,\ldots\qquad\lambda>0\,.
$$ 
b. Simule $N=100,000$ muestras independientes e idénticamente distribuidas de $y$ con $\lambda = 1$, y compare la distribución empírica correspondiente con la distribución exacta obtenida en el numeral anterior.
    
2. Muestre que si $y\mid\theta$ tiene distribución exponencial con parámetro $\theta$, entonces la distribución $\textsf{Gamma}$ sirve como distribución previa conjugada para hacer inferencias sobre $\theta$, dada una muestra aleatoria de valores de $y$.

2. Suponga que Su estado de información previo para $\theta$, la proporción de individuos que apoyan la pena de muerte en California, es $\textsf{Beta}$ con media $\textsf{E}(\theta) = 0.6$ y desviación estándar $\textsf{DE}(\theta) = 0.3$.

a. Determine los hiperparámetros de Su distribución previa y dibuje la función de densidad correspondiente.
b. Se toma una muestra aleatoria de 1,000 californianos y el 65\% apoya la pena de muerte. Calcule tanto la media como la desviación estándar posterior para $\theta$. Dibuje la función de densidad posterior correspondiente.
c. Examine la sensibilidad de la distribución posterior a diferentes valores de la media y de la desviación estándar a priori, incluyendo una distribución previa no informativa.

2. Un ingeniero está inspeccionando un gran envío de piezas con fines de control de calidad y decide probar diez elementos seleccionados al azar. Históricamente, la proporción de artículos defectuosos $\theta$ ha sido de alrededor del 1\% y muy rara vez ha estado por encima del 2\%.

a. Determine una distribución previa conjugada para $\theta$ de acuerdo con la información histórica, y además, usando esta distribución previa, encuentre la distribución posterior de $\theta$ dada una muestra aleatoria de tamaño diez.
b. Suponga que el ingeniero no encuentra componentes defectuosos en su proceso de observación. ¿Cuál es la distribución posterior de $\theta$? ¿Cuál es la media posterior de $\theta$?
c. Calcule el estimador de máxima verosimilitud para $\theta$. Como estimador puntual	, ¿en este caso es preferible el estimador máximo verosímil o la media posterior? ¿Por qué?
    
2. Jeffreys (**H. Jeffreys (1961). Theory of Probability. Oxford Univ. Press.**) sugirió una regla para generar una distribución previa de un parámetro $\theta$ asociado con la distribución muestral $p(y\mid\theta)$. La distribución previa de Jeffreys es de la forma $p_J(\theta)\propto\sqrt{I(\theta)}$, donde
$$
I(\theta) = -\textsf{E}_{y\mid\theta}\left( \frac{\text{d}^2}{\text{d}\theta^2}\log p(y\mid\theta) \right)
$$
es la información esperada de Fisher.
	
a. Sea $y\mid\theta\sim\textsf{Bin}(n,\theta)$. Muestre que la distribución previa de Jeffreys para esta distribución muestral es 
$$
p_J(\theta)\propto \theta^{-\frac12} (1-\theta)^{-\frac12}\,.
$$
b. Reparametrice la distribución Binomial con $\psi = \textsf{logit}(\theta)$, de forma que 
$$p(y\mid\psi) \propto e^{\psi y}(1+e^\psi)^{-n}\,.$$
Obtenga la distribución previa de Jeffreys para esta distribución muestral.
b. Tome la distribución previa de la parte a. y aplique la fórmula del cambio de variables para obtener la densidad previa de $\psi$. Esta densidad debe coincidir con la obtenida en el inciso b.. Esta propiedad de invarianza bajo reparametrización es la característica fundamental de la previa de Jeffreys.

2. El estimador óptimo del parámetro $\theta\in \Theta\subset\mathbb{R}$ de acuerdo con la regla de Bayes se define como el estimador $\hat\theta=\hat\theta(\boldsymbol{y})$ que minimiza la perdida esperada posterior dada por
$$
\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)) = \int_{\Theta} L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, \textsf{d}\theta\,,
$$
donde $L(\theta,\hat\theta)$ es una función de perdida (costo que conlleva estimar $\theta$ por medio de $\hat\theta$) y $\boldsymbol{y}=(y_1,\ldots, y_n)$ es un conjunto de datos observado.

a. Muestre que si $L(\theta,\hat\theta) = (\theta - \hat\theta)^2$, entonces el estimador óptimo de acuerdo con la regla de Bayes es la media posterior $\hat\theta=\textsf{E}(\theta\mid\boldsymbol{y})$.
b. Muestre que si $L(\theta,\hat\theta) = |\theta - \hat\theta|$, entonces el estimador óptimo de acuerdo con la regla de Bayes es la mediana posterior $\hat\theta=(\theta\mid\boldsymbol{y})_{0.5}$.
c. El riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ se define como
$$
R_{\textsf{F}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}\mid\theta}(L(\theta,\hat\theta)) = \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\boldsymbol{y}\mid\theta)\,\textsf{d}\boldsymbol{y}\,,
$$
i.e., el valor medio de la función de perdida $L(\theta,\hat\theta)$ a través de todos los valores de $\boldsymbol{y} \in \mathcal{Y}$. De otra parte, el riesgo Bayesiano $R_{\textsf{B}}(\theta,\hat\theta)$ se define como
$$
R_{\textsf{B}}(\theta,\hat\theta) = \textsf{E}_{\theta}(R_{\textsf{F}}(\theta,\hat\theta)) = \int_{\Theta} R_{\textsf{F}}(\theta,\hat\theta)\, p(\theta)\,\textsf{d}\theta\,,
$$
i.e., el valor medio del riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ a priori a través de todos los valores de $\theta\in\Theta$. Muestre que
$$
R_{\textsf{B}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}}(\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)))\,,
$$
donde $\textsf{E}_{\boldsymbol{y}}(\cdot)$ denota el valor esperado respecto a la distribución marginal $p(\boldsymbol{y})$.

# Solución {-}

1. Sean $x$, $y$, y $z$ variables aleatorias con función de densidad conjunta (discreta o continua) dada por $p(x,y,z) \propto p(x,z)p(y,z)p(z)$. Muestre que:

a. $p(x\mid y,z)\propto p(x,z)$, i.e., $p(x\mid y,z)$ es función de $x$ y $z$.
        
Se tiene que 
$$
p(x\mid y,z) = \frac{p(x,y,z)}{p(y,z)} = \frac{p(x,z)p(y,z)p(z)}{p(y,z)} \propto p(x,z)\,,
$$
y por lo tanto $p(x\mid y,z)$ es función de $x$ y $z$.

b. $p(y\mid x,z)\propto p(y,z)$, i.e., $p(y\mid x,z)$ es función de $y$ y $z$.
    
Se tiene que 
$$
p(y\mid x,z) = \frac{p(x,y,z)}{p(x,z)} = \frac{p(x,z)p(y,z)p(z)}{p(x,z)} \propto p(y,z)\,,
$$
y por lo tanto p(y\mid x,z)$ es función de $y$ y $z$.
    
b. $x$ y $y$ son condicionalmente independientes dado $z$.
    
Se tiene que 
$$
p(x,y\mid z) = \frac{p(x,y,z)}{p(z)} = \frac{p(x,z)p(y,z)p(z)}{p(z)} = \frac{p(x,z)p(z)}{p(z)}\,\frac{p(y,z)p(z)}{p(z)} \propto p(x\mid z)\,p(y\mid z)\,,
$$
y por lo tanto $x$ y $y$ son condicionalmente independientes dado $z$.
    
2. Sean $A$, $B$, y $C$ proposiciones de falso-verdadero. Suponga que $A$ y $B$ son condicionalmente independientes, dado $C$. Muestre que:

a. $A$   y $B^C$ son condicionalmente independientes, dado $C$.	
        
Si $A$ y $B$ son condicionalmente independientes, dado $C$, entonces $\textsf{Pr}(B\mid A,C) = \textsf{Pr}(B\mid C)$, y por lo tanto $1-\textsf{Pr}(B\mid A,C) = 1-\textsf{Pr}(B\mid C)$, y así, $\textsf{Pr}(B^C\mid A,C) = \textsf{Pr}(B^C\mid C)$. Luego, $A$   y $B^C$ son condicionalmente independientes, dado $C$.
        
b. $A^C$ y $B^C$ son condicionalmente independientes, dado $C$.
    
$A^C$ y $B^C$ son condicionalmente independientes, dado $C$, dado que
$$
\begin{align*}
\textsf{Pr}(A^C\cap B^C\mid C) &= \textsf{Pr}((A\cup B)^C\mid C)\\
&= 1-(\textsf{Pr}(A\mid C)+\textsf{Pr}(B\mid C)-\textsf{Pr}(A\cap B\mid C))\\
&= 1-\textsf{Pr}(A\mid C)-\textsf{Pr}(B\mid C))+\textsf{Pr}(A\mid C)\textsf{Pr}(B\mid C)\,.
\end{align*}
$$
Luego, $\textsf{Pr}(A^C\cap B^C\mid C) = 1-\textsf{Pr}(A\mid C)-\textsf{Pr}(B\mid C)(1 - \textsf{Pr}(A\mid C))= (1 - \textsf{Pr}(A\mid C))(1 - \textsf{Pr}(B\mid C))$,
de donde se sigue que $A^C$ y $B^C$ son condicionalmente independientes, dado $C$.

2. Sea $y\mid x\sim\textsf{Poi}(x)$ y $x\mid\lambda\sim\textsf{Exp}(\lambda)$.
    
a. Muestre que la distribución marginal de $y$ es:
$$
p(y\mid\lambda) = \frac{\lambda}{(\lambda+1)^{y+1}}\,,\qquad y = 0,1,\ldots\qquad\lambda>0\,.
$$ 
		
La distribución marginal de $y$ está dada por:
\begin{align*}
		  p(y) & = \int_{\mathcal{X}} p(y,x)\,\text{d}x \\
		  &= \int_{\mathcal{X}} p(y\mid x) p (x)\,\text{d}x \\
		  &= \int_{0}^{\infty} \frac{e^{-x} x^y}{y!} \lambda e^{-\lambda x}\,\text{d}x \\
		  &= \frac{\lambda}{y!} \int_{0}^{\infty} x^y e^{-(\lambda+1)x}\,\text{d}x \\
		  &= \frac{\lambda}{y!} \frac{\Gamma(y+1)}{(\lambda+1)^{y+1}}\\
		  &= \frac{\lambda}{(\lambda+1)^{y+1}}\,,
\end{align*}
dado que $x^y e^{-(\lambda+1)x}$ corresponde al núcleo de una distribución Gamma con parámetros $\alpha=y+1$ y $\beta=\lambda+1$, y además, $\Gamma(y+1)=y!$ dado que $y\in\{0,1,\ldots\}$.
		
b. Simule $N=100,000$ muestras independientes e idénticamente distribuidas de $y\mid\lambda$ con $\lambda = 1$, y compare la distribución empírica correspondiente con la distribución exacta obtenida en el numeral anterior.
          
```{r, fig.width=6, fig.height=6}
# simulacion
N <- 100000
lambda <- 1
set.seed(1)
x_sim <- rexp(n = N, rate = lambda)
y_sim <- rpois(n = N, lambda = x_sim)
py_sim <- table(y_sim)/N
# p(y)
y <- 0:(length(py_sim)-1)
py_exa <- lambda/(lambda+1)^{y+1}
# grafico
plot(x = y-0.1, y = py_exa, type = "h", lwd = 4, col = "blue", xlab = "y", ylab = "p(y)", main = "Distr. marginal de y")
lines(x = y+0.1, y = py_sim, type = "h", lwd = 4, col = "red")
legend("topright", legend = c("Simulación","Exacta"), col = c("red","blue"), lwd = 2, bty = "n")
# tabla
tab <- round(cbind(py_sim, py_exa, abs(py_sim - py_exa)), 4)
colnames(tab) <- c("Simulación", "Exacta", "Diferencia")
print(head(tab, 10))
```

2. Muestre que si $y\mid\theta$ se distribuye exponencialmente con parámetro $\theta$, entonces la distribución $\textsf{Gamma}$ sirve como distribución previa conjugada para hacer inferencias sobre $\theta$, dada una muestra aleatoria de valores de $y$.

Si $y_i\stackrel{\text{iid}}{\sim}\theta\sim \textsf{Exp}(\theta)$, para $i=1,\ldots,n$,  y $\theta\sim\textsf{Gamma}(a,b)$, entonces la distrbución posterior de $\theta$ es:
\begin{align*}
    p(\theta\mid\boldsymbol{y}) &\propto p(\boldsymbol{y}\mid\theta)\,p(\theta)\\
    &= \prod_{i=1}^n \textsf{Exp}(y_i\mid\theta) \,\textsf{Gamma}(\theta)\\
    &= \prod_{i=1}^n \theta e^{-\theta\,y_i} \,\frac{b^a}{\Gamma(a)}\,\theta^{a-1}\,e^{-b\theta}\\
    &\propto \theta^n e^{-\theta s}\theta^{a-1}\,e^{-b\theta}\\
    &= \theta^{(a+n)-1} e^{-(b+s)\theta}\,,
\end{align*}
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$ y $s=\sum_{i=1}^n y_i$. Por lo tanto $\theta\mid\boldsymbol{y} \sim \textsf{Gamma}(a + n, b +s)$ dado que $\theta^{(a+n)-1} e^{-(b+s)\theta}$ corresponde al núcleo de una distribución Gamma con parámatros $\alpha=a+n$ y $\beta = b + s$. Así, la distribución Gamma sirve como distribución previa conjugada para hacer inferencias sobre $\theta$ porque la distribución posterior de $\theta$ también pertenece a la familia de distribuciones Gamma. 

2. Suponga que Su estado de información previo para $\theta$, la proporción de individuos que apoyan la pena de muerte en California, es $\textsf{Beta}$ con media $\textsf{E}(\theta) = 0.6$ y desviación estándar $\textsf{DE}(\theta) = 0.3$.

a. Determine los hiperparámetros de Su distribución previa y dibuje la función de densidad correspondiente.
    
La información externa indica que $\textsf{E}(\theta) = \frac{a}{a+b} =  0.6$ y $\textsf{Var}(\theta) = \frac{ab}{(a+b)^2(a+b+1)} = 0.3^2$, el cual corresponde a un sistema de ecuaciones para $a$ y $b$. Así, se tiene que $a+b = 5a/3$ de donde $b = 2a/3$ y $\frac{2a/3}{(25a/9)(5a/3+1)} = 0.3^2$. Despejando se tiene que $a = 1$ y por lo tanto $b = 2/3$.
    
```{r, fig.width=6, fig.height=6}
# hiperparametro a
a <- (3/5)*((2/3)/((25/9)*0.3^2) - 1)
a
# hiperparametro b
b <- 2*a/3
b
# media
a/(a+b)
# varianza
a*b/((a+b)^2*(a+b+1))
# grafico de la previa
curve(expr = dbeta(x, shape1 = a, shape2 = b), from = 0.001, to = 0.999, lwd = 2, xlab = expression(theta), ylab = expression(paste("p","(",theta,")",sep="")), main = "Distr. previa")
```
    
b. Se toma una muestra aleatoria de 1,000 californianos y el 65\% apoya la pena de muerte. Calcule tanto la media como la desviación estándar posterior para $\theta$. Dibuje la función de densidad posterior correspondiente.
    
La distribución posterior de $\theta$ es $\theta\mid s \sim\textsf{Beta}(a + s, b + n -s)$. Dado que $a = 1$, $b = 2/3$, $n=1000$ y $s=\sum_{i=1}^n y_i=650$, entonces $\textsf{E}(\theta\mid s) = 0.6499$ y $\textsf{DE}(\theta\mid s) = 0.0151$.
        
```{r, fig.width=12, fig.height=6}
# distribucion posterior
a <- 1
b <- 2/3
n <- 1000
s <- 0.65*n
ap <- a + s
bp <- b + n - s
# media posterior
round(ap/(ap + bp), 4)
# DE posterior
round(sqrt(ap*bp/((ap+bp)^2*(ap+bp+1))), 4)
# grafico de la posterior
par(mfrow = c(1,2))
curve(expr = dbeta(x, shape1 = ap, shape2 = bp), from = 0.001, to = 0.999, lwd = 2, xlab = expression(theta), ylab = expression(paste("p","(",theta," | ",y,")",sep="")), main = "Distr. posterior")
curve(expr = dbeta(x, shape1 = ap, shape2 = bp), from = 0.6, to = 0.7, lwd = 2, xlab = expression(theta), ylab = expression(paste("p","(",theta," | ",y,")",sep="")), main = "Distr. posterior")
```

c. Examine la sensibilidad de la distribución posterior a diferentes valores de la media y de la desviación estándar a priori, incluyendo una distribución previa no informativa.
    
Se observa que utilizando la previa uniforme o la previa de Jeffreys los resultados a posteriori son casi idénticos. Esto ocurre porque el tamaño de muestra es considerablemente grande, lo que opaca la información proveniente de la distribución previa. 
          
```{r, fig.width=6, fig.height=6}
# previa original
a  <- 1
b  <- 2/3
# previa uniforme
a1 <- 1
b1 <- 1
# previa Jeffreys
a2 <- 1/2
b2 <- 1/2
# datos
n  <- 1000
s  <- 0.65*n
# grafico de la posterior
curve(expr = dbeta(x, shape1 = a + s, shape2 = b + n - s), from = 0.6, to = 0.7, lwd = 2, lty = 1, xlab = expression(theta), ylab = expression(paste("p","(",theta," | ",y,")",sep="")), main = "Distr. posterior")
curve(expr = dbeta(x, shape1 = a1 + s, shape2 = b1 + n - s), from = 0.6, to = 0.7, lwd = 2, col = 2, lty = 2, add = T)
curve(expr = dbeta(x, shape1 = a2 + s, shape2 = b2 + n - s), from = 0.6, to = 0.7, lwd = 2, col = 3, lty = 3, add = T)
legend("topright", legend = c("a = 1, b = 2/3","a = 1, b = 1","a = 1/2, b = 1/2"), col = c(1,2,3), lwd = 2, lty = c(1,2,3), bty = "n")
```

2. Un ingeniero está inspeccionando un gran envío de piezas con fines de control de calidad y decide probar diez elementos seleccionados al azar. Históricamente, la proporción de artículos defectuosos $\theta$ ha sido de alrededor del 1\% y muy rara vez ha estado por encima del 2\%.

a. Determine una distribución previa conjugada para $\theta$ de acuerdo con la información histórica, y además, usando esta distribución previa, encuentre la distribución posterior de $\theta$ dada una muestra aleatoria de tamaño diez.
    
A priori se sabe que $\textsf{E}(\theta)=0.01$ y $\textsf{Pr}(\theta < 0.02) \approx 0.97$. Este corresponde a un sistema de ecuaciones no lineales de dos incognitas con infinitas soluciones. Por ejemplo, tomando $a=5$ y $b=495$ se obtenienen las restricciones requeridas a priori (por su puesto otras soluciones también son posibles). 

Usando la distribución previa $\theta\sim\textsf{Beta}(5,495)$ se tiene que la distribución posterior de $\theta$ para una muestra de tamaño $n=10$ es $\theta\mid s \sim\textsf{Beta}(5 + s, 505 - s)$.
       
```{r}
# hiperparametros
a <- 5
b <- 0.99*a/0.01
b
# media
a/(a+b)
# acumulada a 0.02 
pbeta(q = 0.02, shape1 = a, shape2 = b, lower.tail = T)
```

b. Suponga que el ingeniero no encuentra componentes defectuosos en su proceso de observación. ¿Cuál es la distribución posterior de $\theta$? ¿Cuál es la media posterior de $\theta$?
    
Si $s=0$ entonces la distribución posterior de $\theta$ es $\theta\mid s \sim\textsf{Beta}(5, 505)$ y por lo tanto la media posterior de $\theta$ es $\textsf{E}(\theta\mid s) = 0.0098$.
        
```{r}
# hiperparametros
a <- 5
b <- 0.99*a/0.01
# posterior
n <- 10
s <- 0
ap <- a + s
bp <- b + n - s
# media posterior
ap/(ap+bp)
```
      
c. Calcule el estimador de máxima verosimilitud para $\theta$. Como estimador puntual	, ¿en este caso es preferible el estimador máximo verosímil o la media posterior? ¿Por qué?

En este caso la estimación máximo verosímil de $\theta$ es $\bar{y}= 0$. La media posterior es un mejor estimador porque, 1. tiene en consideración la información histórica acerca de las componentes defectuosas, y 2. no tiene sentido estimar la proporción poblacional de componentes defectuosos como cero dado que aunque las componentes defectuosas son pocas claramente a nivel poblacional no son exactamente iguales a cero.
  
    
2. Jeffreys (**H. Jeffreys (1961). Theory of Probability. Oxford Univ. Press.**) sugirió una regla para generar una distribución previa de un parámetro $\theta$ asociado con la distribución muestral $p(y\mid\theta)$. La distribución previa de Jeffreys es de la forma $p_J(\theta)\propto\sqrt{I(\theta)}$, donde
$$
I(\theta) = -\textsf{E}_{y\mid\theta}\left( \frac{\text{d}^2}{\text{d}\theta^2}\log p(y\mid\theta) \right)
$$
es la información esperada de Fisher.
    
a. Sea $y\mid\theta\sim\textsf{Bin}(n,\theta)$. Muestre que la distribución previa de Jeffreys para esta distribución muestral es 
$$
p_J(\theta)\propto \theta^{-\frac12} (1-\theta)^{-\frac12}\,.
$$

Si $y\mid\theta\sim\textsf{Bin}(n,\theta)$, entonces $p(y\mid\theta) = \binom{n}{y}\theta^y(1-\theta)^{n-y}$, y por lo tanto $\log p(y\mid\theta) = \log\binom{n}{y} + y\log\theta + (n-y)\log(1-\theta)$, de donde
$$
\frac{\text{d}}{\text{d}\theta}\log p(y\mid\theta) = \frac{y}{\theta} - \frac{n-y}{1-\theta}
\qquad\text{y}\qquad
\frac{\text{d}^2}{\text{d}\theta^2}\log p(y\mid\theta)=-\left(\frac{y}{\theta^2}+\frac{n-y}{(1-\theta)^2}\right)\,.
$$
Así,
$$
I(\theta) = -\textsf{E}_{y\mid\theta}\left( \frac{\text{d}^2}{\text{d}\theta^2}\log p(y\mid\theta) \right)
= -\textsf{E}_{y\mid\theta}\left( -\left(\frac{y}{\theta^2}+\frac{n-y}{(1-\theta)^2}\right) \right) 
= \frac{\textsf{E}_{y\mid\theta}(y)}{\theta^2}+\frac{n-\textsf{E}_{y\mid\theta}(y)}{(1-\theta)^2}
= \frac{n\theta}{\theta^2}+\frac{n-n\theta}{(1-\theta)^2} = \frac{n}{\theta(1-\theta)}\,.
$$
Entonces, 
$$
p_J(\theta) \propto \sqrt{I(\theta)} = \sqrt{\frac{n}{\theta(1-\theta)}} \propto \frac{1}{\sqrt{\theta(1-\theta)}} = \theta^{-1/2}(1-\theta)^{-1/2}\,.
$$

b. Reparametrice la distribución Binomial con $\psi = \textsf{logit}(\theta)$, de forma que 
$$p(y\mid\psi) \propto e^{\psi y}(1+e^\psi)^{-n}\,.$$
Obtenga la distribución previa de Jeffreys para esta distribución muestral.

Si $\psi = \textsf{logit}(\theta)=\log\frac{\theta}{1-\theta}$, entonces $\theta = \frac{e^\psi}{1+e^\psi}$. Por lo tanto,
$$
p(y\mid\psi) = \binom{n}{y}\left( \frac{e^\psi}{1+e^\psi} \right)^y\left(1-\frac{e^\psi}{1+e^\psi}\right)^{n-y} = \binom{n}{y}\, \frac{e^{\psi y}}{(1+e^\psi)^y}\,\frac{1}{(1+e^\psi)^{n-y}}  = \binom{n}{y} e^{\psi y}(1+e^\psi)^{-n}\,.
$$
Luego, $\log p(y\mid\psi) = \log\binom{n}{y} + \psi y -n\log(1+e^\psi)$, de donde
$$
\frac{\text{d}}{\text{d}\psi}\log p(y\mid\psi) = y - n\,\frac{e^\psi}{1+e^\psi}
\qquad\text{y}\qquad
\frac{\text{d}^2}{\text{d}\psi^2}\log p(y\mid\psi)=-n\,\frac{e^\psi}{(1+e^\psi)^2}\,.
$$
Así,
$$
I(\psi) = -\textsf{E}_{y\mid\psi}\left( \frac{\text{d}^2}{\text{d}\psi^2}\log p(y\mid\psi) \right)
= -\textsf{E}_{y\mid\psi}\left( -n\,\frac{e^\psi}{(1+e^\psi)^2} \right) 
= n\,\frac{e^\psi}{(1+e^\psi)^2}\,.
$$
Entonces, 
$$
p_J(\psi) \propto \sqrt{I(\psi)} = \sqrt{n\,\frac{e^\psi}{(1+e^\psi)^2}} \propto \frac{e^{\psi/2}}{1+e^\psi}\,.
$$

b. Tome la distribución previa de la parte a. y aplique la fórmula del cambio de variables para obtener la densidad previa de $\psi$. Esta densidad debe coincidir con la obtenida en el inciso b.. Esta propiedad de invarianza bajo reparametrización es la característica fundamental de la previa de Jeffreys.

Si $p_J(\theta) \propto \theta^{-1/2}(1-\theta)^{1-/2}$ y $\psi = \textsf{logit}(\theta)=\log\frac{\theta}{1-\theta}$, entonces aplicando la fórmula del cambio de variables (e.g., https://online.stat.psu.edu/stat414/lesson/22/22.2) se tiene que $\theta = \frac{e^\psi}{1+e^\psi}$ y $\frac{\textsf{d}\theta}{\textsf{d}\psi}=e^{-\psi}/(1+e^{-\psi})^2$, de donde
$$
p_J(\psi) = p_J(\theta)\,\Big|\frac{\textsf{d}\theta}{\textsf{d}\psi}\Big|= \left(  \frac{e^\psi}{1+e^\psi} \right)^{-1/2}\left(1- \frac{e^\psi}{1+e^\psi}\right)^{-1/2}\,\frac{e^{-\psi}}{(1+e^{-\psi})^2} = \frac{e^{\psi/2}}{1+e^\psi}\,.
$$

2. El estimador óptimo del parámetro $\theta\in \Theta\subset\mathbb{R}$ de acuerdo con la regla de Bayes se define como el estimador $\hat\theta=\hat\theta(\boldsymbol{y})$ que minimiza la perdida esperada posterior dada por
$$
\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)) = \int_{\Theta} L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, \textsf{d}\theta\,,
$$
donde $L(\theta,\hat\theta)$ es una función de perdida (costo que conlleva estimar $\theta$ por medio de $\hat\theta$) y $\boldsymbol{y}=(y_1,\ldots, y_n)$ es un conjunto de datos observado.

a. Muestre que si $L(\theta,\hat\theta) = (\theta - \hat\theta)^2$, entonces el estimador óptimo de acuerdo con la regla de Bayes es la media posterior $\hat\theta=\textsf{E}(\theta\mid\boldsymbol{y})$.

Diferenciado $\textsf{E}_{\theta\mid\boldsymbol{y}}((\theta-\hat\theta)^2)$ respecto a $\hat\theta$, se tiene que
$$
\frac{\textsf{d}}{\textsf{d}\hat\theta}\textsf{E}_{\theta\mid\boldsymbol{y}}\left((\theta-\hat\theta)^2\right)=\frac{\textsf{d}}{\textsf{d}\hat\theta}\textsf{E}_{\theta\mid\boldsymbol{y}}\left(\theta^2-2\theta\hat\theta+\hat{\theta}^2\right) = -2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta) + 2\hat\theta\,
$$
y por lo tanto el punto crítico correspondiente es $\hat\theta = \textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)$. Así, por el criterio de la segunda derivada, como
$$
\frac{\textsf{d}^2}{\textsf{d}\hat\theta^2}\textsf{E}_{\theta\mid\boldsymbol{y}}\left((\theta-\hat\theta)^2\right)= 2 > 0
$$
para todo $\hat\theta$, entonces en efecto la media posterior $\hat\theta = \textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)$ minimiza $\textsf{E}_{\theta\mid\boldsymbol{y}}((\theta-\hat\theta)^2)$.

b. Muestre que si $L(\theta,\hat\theta) = |\theta - \hat\theta|$, entonces el estimador óptimo de acuerdo con la regla de Bayes es la mediana posterior $\hat\theta=(\theta\mid\boldsymbol{y})_{0.5}$.

Diferenciado $\textsf{E}_{\theta\mid\boldsymbol{y}}(|\theta-\hat\theta|)$ respecto a $\hat\theta$ aplicando la regla de Leibniz (e.g., https://mathworld.wolfram.com/LeibnizIntegralRule.html), se tiene que
$$
\frac{\textsf{d}}{\textsf{d}\hat\theta}\textsf{E}_{\theta\mid\boldsymbol{y}}\left(|\theta-\hat\theta|\right)=\frac{\textsf{d}}{\textsf{d}\hat\theta}\int_\Theta |\theta-\hat\theta|\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = \frac{\textsf{d}}{\textsf{d}\hat\theta}\left( \int_{-\infty}^{\hat\theta} (\hat\theta-\theta)\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta + \int_{\hat\theta}^\infty (\theta-\hat\theta)\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta \right)
= \int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta - \int_{\hat\theta}^\infty \,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta\,,
$$
y por lo tanto, el punto crítico satisface que
$$
 \int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = \int_{\hat\theta}^\infty \hat\theta\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta
 \quad\text{de donde}\quad
 2\int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = \int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta+\int_{\hat\theta}^\infty p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = 1\,,
$$
esto es, $\int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta =1/2$. Luego, el punto crítico correspondiente es $\hat\theta = (\theta\mid\boldsymbol{y})_{0.5}$.  Así, por el criterio de la segunda derivada, como
$$
\frac{\textsf{d}^2}{\textsf{d}\hat\theta^2}\textsf{E}_{\theta\mid\boldsymbol{y}}\left(|\theta-\hat\theta|\right)=  2 > 0    
$$
para todo $\hat\theta$, entonces en efecto la mediana posterior $\hat\theta = (\theta\mid\boldsymbol{y})_{0.5}$ minimiza $\textsf{E}_{\theta\mid\boldsymbol{y}}(|\theta-\hat\theta|)$.

c. El riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ se define como
$$
R_{\textsf{F}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}\mid\theta}(L(\theta,\hat\theta)) = \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\boldsymbol{y}\mid\theta)\,\textsf{d}\boldsymbol{y}\,,
$$
i.e., el valor medio de la función de perdida $L(\theta,\hat\theta)$ a través de todos los valores de $\boldsymbol{y} \in \mathcal{Y}$. De otra parte, el riesgo Bayesiano $R_{\textsf{B}}(\theta,\hat\theta)$ se define como
$$
R_{\textsf{B}}(\theta,\hat\theta) = \textsf{E}_{\theta}(R_{\textsf{F}}(\theta,\hat\theta)) = \int_{\Theta} R_{\textsf{F}}(\theta,\hat\theta)\, p(\theta)\,\textsf{d}\theta\,,
$$
i.e., el valor medio del riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ a priori a través de todos los valores de $\theta\in\Theta$. Muestre que
$$
R_{\textsf{B}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}}(\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)))\,,
$$
donde $\textsf{E}_{\boldsymbol{y}}(\cdot)$ denota el valor esperado respecto a la distribución marginal $p(\boldsymbol{y})$.

Se tiene que
$$
\begin{align*}
R_{\textsf{B}}(\theta,\hat\theta) &= \int_{\Theta} R_{\textsf{F}}(\theta,\hat\theta)\, p(\theta)\,\textsf{d}\theta \\
&= \int_{\Theta} \left( \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\boldsymbol{y}\mid\theta)\,\textsf{d}\boldsymbol{y} \right)\, p(\theta)\,\textsf{d}\theta \\
&= \int_{\Theta} \left( \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\boldsymbol{y}\mid\theta)\, p(\theta)\,\textsf{d}\boldsymbol{y} \right)\,\textsf{d}\theta \\
&= \int_{\Theta} \left( \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, p(\boldsymbol{y})\,\textsf{d}\boldsymbol{y} \right)\,\textsf{d}\theta \\
&= \int_{\mathcal{Y}} \left( \int_{\Theta}  L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, p(\boldsymbol{y})\, \textsf{d}\theta \right)\textsf{d}\boldsymbol{y} \\
&= \int_{\mathcal{Y}} \left( \int_{\Theta}  L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, \textsf{d}\theta \right)\, p(\boldsymbol{y})\,\textsf{d}\boldsymbol{y} \\
&= \int_{\mathcal{Y}} \textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta))\, p(\boldsymbol{y})\,\textsf{d}\boldsymbol{y} \\
&= \textsf{E}_{\boldsymbol{y}}(\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)))\,.
\end{align*}
$$
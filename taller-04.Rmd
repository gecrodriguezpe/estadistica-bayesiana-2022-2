---
title: "Taller 4"
author: 
- Webpage https://sites.google.com/view/juansosa/ 
- YouTube https://www.youtube.com/c/JuanSosa1702 
- GitHub  https://github.com/jstats1702 
- Rpubs   https://rpubs.com/jstats1702
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Suponga que para un problema de respuesta binaria se planea usar una previa uniforme para la proporción de la población $\theta$, con el fin de no favorecer ningún valor de $\theta$ a priori. Sin embargo, algunas personas prefieren estudiar las proporciones en escala logit, es decir, están interesados en $\gamma = \log\frac{\theta}{1-\theta}$. Vía simulación de Monte Carlo, encuentre la distribución previa de $\gamma$ inducida por la distribución uniforme para $\theta$. ¿Esta distribución es Uniforme para $\gamma$?
	
2. Se quiere comparar dos ciudades cuyos sistemas de opinión se pueden considerar como independientes, en términos de las tasas de apoyo $\theta_1$ y $\theta_2$ que los ciudadanos otorgan a una medida económica gubernamental. Por tal motivo, se realiza un estudio de carácter observacional en el que, entre otras variables, se recopilan datos sobre la variable binaria $y_{i,j}$ que asume el valor 1 si la persona $i$ de la ciudad $j$ apoya la medida, y asume el valor 0 en caso contrario, para $i=1,\ldots,n_j$ y $j=1,2$. Teniendo en cuenta que $s_1=\sum_{i=1}^{85} y_{i,1} = 57$ y $s_2=\sum_{i=1}^{90} y_{i,2} = 36$, y asumiendo distribuciones previas no informativas para $\theta_1$ y $\theta_2$ en modelos Beta-Binomial independientes, calcule: i. la media de $\theta_1-\theta_2$, ii. un intervalo de credibilidad al 95\% para $\theta_1-\theta_2$, y iii. la probabilidad de que $\theta_1 > \theta_2$, con base en $B=10,000$ muestras independientes de la distribución posterior de $(\theta_1,\theta_2)$. ¿Hay suficiente evidencia empírica para argumentar diferencias significativas entre las tasas de opinión de las dos ciudades?
	
3. Considere el contexto del ejercicio 2 del Taller 3 acerca de las tasas **tumorigenesis** en dos cepas de ratones.

a. Usando la distribución previa de la parte a), calcule $\textsf{Pr}(\theta_{\text{B}} < \theta_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}})$ vía simulación de Monte Carlo.
b. Para cada $m\in\{1,2,\ldots,50\}$, calcule nuevamente $\textsf{Pr}(\theta_{\text{B}} < \theta_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}})$ vía simulación de Monte Carlo, con $\theta_{\text{A}}\sim\textsf{Gamma}(120,10)$ y $\theta_{\text{B}}\sim \textsf{Gamma}(12m,m)$. ¿Qué tan sensitivas son las inferencias acerca del evento $\theta_{\text{B}} < \theta_{\text{A}}$ respecto a la distribución previa de $\theta_{\text{B}}$?
c. Repita los numerales a. y b. reemplazando el evento $\theta_{\text{B}} < \theta_{\text{A}}$ por el evento $\bar{y^*}_{\text{B}} < \bar{y^*}_{\text{A}}$, donde $\bar{y^*}_{\text{A}}$ y $\bar{y^*}_{\text{B}}$ son promedios muestrales calculados a partir de muestras i.i.d. de tamaños 10 y 13 de la distribución predictiva posterior de A y B,  respectivamente.
d. Usando la distribución previa de la parte a., para ambas cepas de ratones, chequee la bondad de ajuste del modelo usando como estadísticos de prueba la media y la desviación estándar.

4. Considere el modelo $y_i\mid\theta \stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2_0)$, para $i = 1,\ldots,n$, con varianza $\sigma^2_0$ conocida, y $\theta \sim \textsf{N}(\mu_0,\tau^2_0)$, donde $\mu_0$ y $\tau^2_0$ son hiperparámetros (conocidos). Muestre que $\theta\mid\boldsymbol{y}\sim\textsf{N}(\mu_n,\tau^2_n)$, donde
$$
\mu_n = \frac{\frac{1}{\tau^2_0}\,\mu_0 + \frac{n}{\sigma^2_0}\,\bar{y}}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2_0}}
\qquad\text{y}\qquad
\tau^2_n = \frac{1}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2_0}}\,,
$$
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$ y $\bar{y} = \frac1n\sum_{i=1}^n y_i$. Así, la media posterior de $\theta$ es un promedio ponderado entre la media a priori $\mu_0$ y la media muestral $\bar{y}$.
	
5. Considere una única observación de la distribución muestral $x\mid\theta\sim\textsf{N}(\theta,\theta)$, con $\theta > 0$. Muestre que la previa de Jeffreys de para $\theta$ es tal que:
$$
p_J(\theta)\propto\frac{(2\theta + 1)^{1/2}}{\theta}\,.
$$

6. Una distribución de la familia exponencial (de un parámetro) es cualquier distribución cuya función de densidad se puede expresar como
$$
p(y\mid\phi) = h(y)\,c(\phi)\,\exp{\left( \phi\,t(y) \right)}\,,
$$ 
donde $\phi$ es el parámetro natural y $t(y)$ es un estadístico suficiente. Muestre que las distribuciones Binomial, Poisson, y Exponencial hacen parte de la familia exponencial. En cada caso identificar el parámetro natural y un estadístico suficiente.

7. La variable aleatoria $X$ tiene distribución Galenshore con parámetros $\alpha,\beta > 0$, i.e., $X\mid\alpha,\beta\sim\textsf{Galenshore}(\alpha,\beta)$, si su función de densidad de probabilidad es
$$
p(x\mid\alpha,\beta) = 
\frac{2}{\Gamma(\alpha)}\,\beta^{2\alpha}\,x^{2\alpha-1}\,e^{-\beta^2x^2}\,,\qquad x>0\,.
$$
Para esta distribución se tiene que 
$$
\textsf{E}(X\mid\alpha,\beta) = \frac{\Gamma(\alpha+\tfrac12)}{\beta\Gamma(\alpha)} \qquad\text{y}\qquad\textsf{E}(X^2\mid\alpha,\beta) = \frac{\alpha}{\beta^2}\,.
$$ 
Asumiendo que $\alpha$ es conocido:
a. Identifique una clase de densidades previas conjugadas para $\beta$.
b. Sea $y_1,\ldots,y_n\mid\beta\stackrel{\text{iid}}{\sim}\textsf{Galenshore}(\alpha,\beta)$. Encuentre la distribución posterior de $\beta$ dado $\boldsymbol{y}=(y_1,\ldots,y_n)$ usando la distribución previa de la clase conjugada del numeral anterior.
c. Identifique un estadístico suficiente para $\beta$ a partir de la distribución condicional conjunta $p(\boldsymbol{y}\mid\beta)$.
d. Determine $\textsf{E}(\beta\mid\boldsymbol{y})$.

8. Suponga que $y_1\ldots,y_5$ son observaciones condicionalmente independientes de una distribución Cauchy con parámetro de localización $\theta$ y parámetro de escala 1, i.e.,
$$
p(y_i\mid\theta) = \frac{1}{\pi(1+(y_i-\theta)^2)}\,,\qquad-\infty<y_i<\infty\,,\qquad-\infty<\theta<\infty\,,
$$
para $i=1,\ldots,5$. Además, asuma por simplicidad que la distribución previa de $\theta$ es Uniforme en el intervalo $(0,100)$, i.e., $\theta\sim\textsf{Unif}(0,100)$. 

Teniendo en cuenta el vector de observaciones $\boldsymbol{y}=(43.0, 44.0, 45.0, 46.5, 47.5)$:

a. Calcule la función de densidad posterior sin normalizar, $p(\boldsymbol{y}\mid\theta)\,p(\theta)$, en una grilla de puntos equidistantes para $\theta$ de la forma $0,\frac{1}{M},\frac{2}{M},\ldots,100$, con $M=1,000$. Usando los valores calculados para cada punto de la grilla, calcule y grafique la función de densidad posterior normalizada, $p(\theta\mid\boldsymbol{y})$.
b. Usando la aproximación discreta del numeral anterior, obtenga $B=10,000$ muestras de la distribución posterior de $\theta$ y grafique el histograma correspondiente (junto con una estimación puntual y un intervalo de credibilidad al 95\%).
c. Utilice las muestras de la distribución posterior de $\theta$ del numeral anterior para obtener muestras de la distribución predictiva posterior de una observación futura y grafique el histograma correspondiente (junto con una estimación puntual y un intervalo de credibilidad al 95\%).

# Solución {-}

1. Suponga que para un problema de respuesta binaria se planea usar una previa uniforme para la proporción de la población $\theta$, con el fin de no favorecer ningún valor de $\theta$ a priori. Sin embargo, algunas personas prefieren estudiar las proporciones en escala logit, es decir, están interesados en $\gamma = \log\frac{\theta}{1-\theta}$. Vía simulación de Monte Carlo, encuentre la distribución previa de $\gamma$ inducida por la distribución uniforme para $\theta$. ¿Esta distribución es Uniforme para $\gamma$?

La distribución (obtenida de manera empírica) de $\gamma = \frac{\theta}{1-\theta}$, con $\theta\sim\textsf{Uniforme(0,1)}$ se presenta en la siguiente Figura. Se observa que la distribución de $\gamma$ es simétrica al rededor de $\gamma = 0$ y que claramente no es uniforme en su dominio. Este ejemplo ilustra como una distribución puede ser uniforme en una escala, pero no en otra. 

```{r}
# simulacion
B <- 100000
set.seed(1234)
theta_mc <- runif(n = B, min = 0, max = 1)
gamma_mc <- log(theta_mc) - log(1 - theta_mc)
```

```{r}
# grafico
par(mfrow = c(1,1), mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
hist(x = gamma_mc, freq = FALSE, col = "gray90", border = "gray90", xlab = expression(gamma), ylab = "Densidad", main = "", xlim = 12*c(-1,1), ylim = c(0,0.25))
lines(density(gamma_mc), col = 1)
```


2. Se quiere comparar dos ciudades cuyos sistemas de opinión se pueden considerar como independientes, en términos de las tasas de apoyo $\theta_1$ y $\theta_2$ que los ciudadanos otorgan a una medida económica gubernamental. Por tal motivo, se realiza un estudio de carácter observacional en el que, entre otras variables, se recopilan datos sobre la variable binaria $y_{i,j}$ que asume el valor 1 si la persona $i$ de la ciudad $j$ apoya la medida, y asume el valor 0 en caso contrario, para $i=1,\ldots,n_j$ y $j=1,2$. Teniendo en cuenta que $s_1=\sum_{i=1}^{85} y_{i,1} = 57$ y $s_2=\sum_{i=1}^{90} y_{i,2} = 36$, y asumiendo distribuciones previas no informativas para $\theta_1$ y $\theta_2$ en modelos Beta-Binomial independientes, calcule: i. la media de $\theta_1-\theta_2$, ii. un intervalo de credibilidad al 95\% para $\theta_1-\theta_2$, y iii. la probabilidad de que $\theta_1 > \theta_2$, con base en $B=10,000$ muestras independientes de la distribución posterior de $(\theta_1,\theta_2)$. ¿Hay suficiente evidencia empírica para argumentar diferencias significativas entre las tasas de opinión de las dos ciudades?

En este caso se tiene que $\theta_1\mid\boldsymbol{y}_1\sim\textsf{Beta}(58,29)$ y $\theta_2\mid\boldsymbol{y}_2\sim\textsf{Beta}(37,55)$, donde $\boldsymbol{y}_j=(y_{1,j},\ldots,y_{2,j})$ contiene los datos binarios de la ciudad $j$, con $j=1,2$. La tabla que se presenta a continuación muestra la media de $\theta_1-\theta_2$, un intervalo de credibilidad al 95\% para $\theta_1-\theta_2$, la probabilidad de que $\theta_1 > \theta_2$, con base en $B=10,000$ muestras independientes de la distribución posterior de $(\theta_1,\theta_2)$. La probabilidad a posteriori de que $\theta_1>\theta_2$ es exesivamente alta (aprox. 100\%), y además, hay suficiente evidencia empírica para argumentar diferencias significativas entre las tasas de opinión de las dos ciudades (ya que el intervalo de credibilidad crrespondiente no contiene a 0). En particular, la tasa de apoyo en el primer grupo es significativamente superior en comparación con el segundo grupo.

```{r, eval=T, echo=F}
# tamaños de muestra
n1 <- 85
n2 <- 90
# estadisticos suficientes
s1 <- 57
s2 <- 36
# hiperparametros
a <- 1
b <- 1
# distribucion posterior
ap1 <- a + s1
bp1 <- b + n1 - s1
ap2 <- a + s2
bp2 <- b + n2 - s2
# simulacion 
B <- 10000
set.seed(1234)
theta1_mc <- rbeta(n = B, shape1 = ap1, shape2 = bp1)
theta2_mc <- rbeta(n = B, shape1 = ap2, shape2 = bp2)
# inferencia
gamma_mc <- theta1_mc - theta2_mc
tab <- as.matrix(c(mean(gamma_mc), quantile(x = gamma_mc, probs = c(.025,.975)), mean(gamma_mc > 0)))
colnames(tab) <- c("theta_1 - theta_2")
rownames(tab) <- c("Estimación","Q2.5%","Q97.5%","Pr(theta_1 > theta_2)")
round(tab, 3)
# grafico
par(mfrow = c(1,1), mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
hist(x = gamma_mc, freq = FALSE, col = "gray95", border = "gray95", xlab = expression(theta[1] - theta[2]), ylab = "Densidad", main = expression(paste("Distr. posterior de ", theta[1] - theta[2])))
lines(density(theta1_mc - theta2_mc), col = "gray", lwd = 2)
abline(v = 0, col = 1, lty = 1)
abline(v = mean(gamma_mc), col = 2, lty = 3)
abline(v = quantile(gamma_mc, c(.025,.975)), col = 4, lty = 3)
legend("topleft", legend = c("Posterior", "Media", "Intervalo al 95%"), col = c("gray",2,4), lwd = 2, bty = "n")
```
```{r, eval=F, echo=T}
# tamaños de muestra
n1 <- 85
n2 <- 90
# estadisticos suficientes
s1 <- 57
s2 <- 36
# hiperparametros
a <- 1
b <- 1
# distribucion posterior
ap1 <- a + s1
bp1 <- b + n1 - s1
ap2 <- a + s2
bp2 <- b + n2 - s2
# simulacion 
B <- 10000
set.seed(1234)
theta1_mc <- rbeta(n = B, shape1 = ap1, shape2 = ap2)
theta2_mc <- rbeta(n = B, shape1 = ap2, shape2 = ap2)
# inferencia
gamma_mc <- theta1_mc - theta2_mc
tab <- as.matrix(c(mean(gamma_mc), quantile(x = gamma_mc, probs = c(.025,.975)), mean(gamma_mc > 0)))
colnames(tab) <- c("theta_1 - theta_2")
rownames(tab) <- c("Estimación","Q2.5%","Q97.5%","Pr(theta_1 > theta_2)")
round(tab, 3)
# grafico
par(mfrow = c(1,1), mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
hist(x = gamma_mc, freq = FALSE, col = "gray95", border = "gray95", xlab = expression(theta[1] - theta[2]), ylab = "Densidad", main = expression(paste("Distr. posterior de ", theta[1] - theta[2])))
lines(density(theta1_mc - theta2_mc), col = "gray", lwd = 2)
abline(v = 0, col = 1, lty = 1)
abline(v = mean(gamma_mc), col = 2, lty = 3)
abline(v = quantile(gamma_mc, c(.025,.975)), col = 4, lty = 3)
legend("topleft", legend = c("Posterior", "Media", "Intervalo al 95%"), col = c("gray",2,4), lwd = 2, bty = "n")
```



	
3. Considere el contexto del ejercicio 2 del Taller 3 acerca de las tasas **tumorigenesis** en dos cepas de ratones.

a. Usando la distribución previa de la parte a), calcule $\textsf{Pr}(\theta_{\text{B}} < \theta_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}})$ vía simulación de Monte Carlo.

En este caso, la distribución posterior en ambos casos es tipo Gamma. En particular, se tiene que las distribuciones posteriores correspondientes son $\theta_A\mid\boldsymbol{y}_A\sim\textsf{Gamma}(237,20)$ y $\theta_B\mid\boldsymbol{y}_B\sim\textsf{Gamma}(125,14)$.

```{r}
# datos
yA <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB <- c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)
# tamaños
nA <- length(yA)
nB <- length(yB)
# estadistico suficiente
sA <- sum(yA)
sB <- sum(yB)
# previa
aA <- 120
bA <- 10
aB <- 12
bB <- 1
# posterior
apA <- aA + sA
bpA <- bA + nA
apB <- aB + sB
bpB <- bB + nB
```

Usando simulación se obtiene que $\textsf{Pr}(\theta_{\text{B}} < \theta_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}}) = 0.9956$.

```{r}
# simulacion posterior
B <- 10000
set.seed(1234)
thA_mc <- rgamma(n = B, shape = apA, rate = bpA)
thB_mc <- rgamma(n = B, shape = apB, rate = bpB)
# probabilidad
round(mean(thB_mc < thA_mc), 4)
```


b. Para cada $m\in\{1,2,\ldots,50\}$, calcule nuevamente $\textsf{Pr}(\theta_{\text{B}} < \theta_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}})$ vía simulación de Monte Carlo, con $\theta_{\text{A}}\sim\textsf{Gamma}(120,10)$ y $\theta_{\text{B}}\sim \textsf{Gamma}(12m,m)$. ¿Qué tan sensitivas son las inferencias acerca del evento $\theta_{\text{B}} < \theta_{\text{A}}$ respecto a la distribución previa de $\theta_{\text{B}}$?

Como se observa en la siguiente Figura, $\textsf{Pr}(\theta_{\text{B}} < \theta_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}})$ cambia radicalmente (decrece) para valores grandes de $m$.

```{r}
# previa A
aA <- 120
bA <- 10
# posterior A
apA <- aA + sA
bpA <- bA + nA
# simulacion A
B <- 10000
set.seed(1234)
thA_mc <- rgamma(n = B, shape = apA, rate = bpA)
# valores de m
mgrid <- 1:50
# calculo de probabilidades para cada m
out <- NULL
set.seed(1234)
for (m in mgrid) {
  # previa B
  aB <- 12*m
  bB <- 1*m
  # posterior B
  apB <- aB + sB
  bpB <- bB + nB
  # simulacion B
  thB_mc <- rgamma(n = B, shape = apB, rate = bpB)
  out[m] <- mean(thB_mc < thA_mc)
}
```

```{r}
# grafico
par(mfrow = c(1,1), mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
plot(x = mgrid, y = out, type = "p", pch = 18, cex = 0.8, ylim = c(0.7, 1), xlab = "m", ylab = "Probabilidad", main = expression(paste("Pr(", theta[B], "<", theta[A], " | ", y[A], ",", y[B], ")")))
```

c. Repita los numerales a. y b. reemplazando el evento $\theta_{\text{B}} < \theta_{\text{A}}$ por el evento $\bar{y^*}_{\text{B}} < \bar{y^*}_{\text{A}}$, donde $\bar{y^*}_{\text{A}}$ y 
$\bar{y^*}_{\text{B}}$ son promedios muestrales calculados a partir de muestras i.i.d. de tamaños 10 y 13 de la distribución predictiva posterior de A y B,  respectivamente.

Usando simulación se obtiene que $\textsf{Pr}(\bar{y^*}_{\text{B}} < \bar{y^*}_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}}) = 0.951$.

```{r}
# datos
yA <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB <- c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)
# tamaños
nA <- length(yA)
nB <- length(yB)
# estadistico suficiente
sA <- sum(yA)
sB <- sum(yB)
# previa
aA <- 120
bA <- 10
aB <- 12
bB <- 1
# posterior
apA <- aA + sA
bpA <- bA + nA
apB <- aB + sB
bpB <- bB + nB
# simulacion posterior
B <- 10000
set.seed(1234)
thA_mc <- rgamma(n = B, shape = apA, rate = bpA)
thB_mc <- rgamma(n = B, shape = apB, rate = bpB)
# probabilidad usando predictiva posterior del promedio
out <- 0
set.seed(1234)
for (b in 1:B)
  out <- out + (mean(rpois(n = nB, lambda = thB_mc[b])) < mean(rpois(n = nA, lambda = thA_mc[b])))/B
round(out, 4)
```
Similarmente a como sucede con las probabilidades a posteriori del evento $\theta_{\text{B}} < \theta_{\text{A}}$, como se observa en la siguiente Figura, $\textsf{Pr}(\bar{y^*}_{\text{B}} < \bar{y^*}_{\text{A}}\mid \boldsymbol{y}_{\text{A}},\boldsymbol{y}_{\text{B}})$ cambia radicalmente (decrece) para valores grandes de $m$.

```{r}
# previa A
aA <- 120
bA <- 10
# posterior A
apA <- aA + sA
bpA <- bA + nA
# simulacion A
B <- 10000
set.seed(1234)
thA_mc <- rgamma(n = B, shape = apA, rate = bpA)
# valores de m
mgrid <- 1:50
# calculo de probabilidades para cada m
tmp <- NULL
set.seed(1234)
for (m in mgrid) {
  # previa B
  aB <- 12*m
  bB <- 1*m
  # posterior B
  apB <- aB + sB
  bpB <- bB + nB
  # simulacion B
  thB_mc <- rgamma(n = B, shape = apB, rate = bpB)
  # probabilidad usando predictiva posterior del promedio
  out <- 0
  for (b in 1:B)
    out <- out + (mean(rpois(n = nB, lambda = thB_mc[b])) < mean(rpois(n = nA, lambda = thA_mc[b])))/B
  tmp[m] <- out
}
```

```{r}
# grafico
par(mfrow = c(1,1), mar = c(3,3,1.8,1.4), mgp = c(1.75,0.75,0))
plot(x = mgrid, y = tmp, type = "p", pch = 18, cex = 0.8, ylim = c(0.6, 1), xlab = "m", ylab = "Probabilidad", main = expression(paste("Pr(", bar(tilde(y))[B], "<", bar(tilde(y))[A], " | ", y[A], ",", y[B], ")")))
```


d. Usando la distribución previa de la parte a., para ambas cepas de ratones, chequee la bondad de ajuste del modelo usando como estadísticos de prueba la media y la desviación estándar.

En la siguiente Figura se observa la distribución predictiva posterior de la media y de la desviación estándar para los dos tipos de ratones A y B, junto con los valores $p$ predictivos posteriores (ppp) correspondientes (los estadísticos observados se representan con lineas negras discontinuas, mientras que los intervalos de credibilidad al 95% con lineas continuas de colores). Se observa que el modelo Gamma-Poisson captura adecuadamente la media y la desviación estándar de los datos observados (valores ppp de 0.53 y 0.62, respectivamente), por lo que se considera como un modelo apropiado para caracterizar estas medidas nivel poblacional. De otra parte, para los ratones tipo B, el modelo ajusta adecuadamente la media (valor ppp de 0.55), pero no la desviación (valor ppp de 0.99). En este últiom caso, dado que los valores predichos son claramente superiores al valor observado, se tiene evidencia empírica de patrones típicos de subdispersión. 

```{r}
# datos
yA <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB <- c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)
# tamaños
nA <- length(yA)
nB <- length(yB)
# estadistico suficiente
sA <- sum(yA)
sB <- sum(yB)
# previa
aA <- 120
bA <- 10
aB <- 12
bB <- 1
# posterior
apA <- aA + sA
bpA <- bA + nA
apB <- aB + sB
bpB <- bB + nB
# simulacion posterior
B <- 10000
set.seed(1234)
thA_mc <- rgamma(n = B, shape = apA, rate = bpA)
thB_mc <- rgamma(n = B, shape = apB, rate = bpB)
# valores observados
tobsA <- c(mean(yA), sd(yA))
tobsB <- c(mean(yB), sd(yB))
# predictiva posterior estadisticos de prueba
outA <- NULL
outB <- NULL
set.seed(1234)
for (b in 1:B) {
  # predictiva
  yA_rep <- rpois(n = nA, lambda = thA_mc[b])
  yB_rep <- rpois(n = nB, lambda = thB_mc[b])
  # estadisticos de prueba
  outA <- rbind(outA, c(mean(yA_rep), sd(yA_rep)))
  outB <- rbind(outB, c(mean(yB_rep), sd(yB_rep)))
}
```

```{r}
par(mfrow = c(2,2), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
tipo <- c("A","B")
nombres <- c("Media","D. Estandar")
colores1 <- c("mistyrose","lightblue1")
colores2 <- c(2, 4) 
for (s in tipo) {
  tobs <- get(x = paste0("tobs", s))
  out  <- get(x = paste0("out",  s))
  for (j in 1:2) {
    ppp <- mean(out[,j] > tobs[j])
    hist(x = out[,j], freq = F, col = colores1[j], border = colores1[j], xlab = nombres[j], ylab = "Densidad", main = paste0("Tipo ", s, ", ppp = ", round(ppp, 2)))
    abline(v = tobs[j], lty = 2)
    abline(v = quantile(out[,j], c(.025,.975)), lty = 1, col = colores2[j])
  }
}
```

4. Considere el modelo $y_i\mid\theta \stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2_0)$, para $i = 1,\ldots,n$, con varianza $\sigma^2_0$ conocida, y $\theta \sim \textsf{N}(\mu_0,\tau^2_0)$, donde $\mu_0$ y $\tau^2_0$ son hiperparámetros (conocidos). Muestre que $\theta\mid\boldsymbol{y}\sim\textsf{N}(\mu_n,\tau^2_n)$, donde
$$
\mu_n = \frac{\frac{1}{\tau^2_0}\,\mu_0 + \frac{n}{\sigma^2_0}\,\bar{y}}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2_0}}
\qquad\text{y}\qquad
\tau^2_n = \frac{1}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2_0}}\,,
$$
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$ y $\bar{y} = \frac1n\sum_{i=1}^n y_i$. Así, la media posterior de $\theta$ es un promedio ponderado entre la media a priori $\mu_0$ y la media muestral $\bar{y}$.

La distribución posterior de $\theta$ es
$$
\begin{align*}
p(\theta\mid\boldsymbol{y}) &\propto \prod_{i=1}^n\textsf{N}(y_i\mid\theta,\sigma^2_0) \times \textsf{N}(\theta\mid\mu_0,\tau^2_0)\\
&= \prod_{i=1}^n(2\pi\sigma_0^2)^{-1/2}\,\textsf{exp}\left\{ \frac{1}{2\sigma^2_0}(y_i-\theta)^2 \right\} \times (2\pi\tau_0^2)^{-1/2}\,\textsf{exp}\left\{ \frac{1}{2\tau^2_0}(\theta-\mu_0)^2 \right\} \\
&\propto \textsf{exp}\left\{ \frac{1}{2\sigma^2_0}\sum_{i=1}^n(y_i-\theta)^2 \right\} \times\textsf{exp}\left\{ \frac{1}{2\tau^2_0}(\theta-\mu_0)^2 \right\} \\
&\propto \textsf{exp}\left\{ \frac{1}{2\sigma^2_0}\left(n\theta^2-2\theta n\bar{y}\right) \right\} \times\textsf{exp}\left\{ \frac{1}{2\tau^2_0}\left(\theta^2-2\theta\mu_0\right) \right\} \\
&= \textsf{exp}\left\{ \frac{1}{2\sigma^2_0}\left(n\theta^2-2\theta n\bar{y}\right) \right\} \times\textsf{exp}\left\{ \frac{1}{2\tau^2_0}\left(\theta^2-2\theta\mu_0\right) \right\} \\
&= \textsf{exp}\left\{ -\frac12\left[ \left(\frac{1}{\tau_0^2}+\frac{n}{\sigma^2_0}\right)\theta^2 - 2\theta\left( \frac{1}{\tau^2_0}\mu_0 + \frac{n}{\sigma^2_0}\bar{y}  \right)  \right] \right\}
\end{align*}
$$
donde $\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$. La anterior expresión corresponde al núcleo de una distribución Normal con media $\mu_n$ y $\tau^2_n$, con
$$
\mu_n = \frac{\frac{1}{\tau^2_0}\,\mu_0 + \frac{n}{\sigma^2_0}\,\bar{y}}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2_0}}
\qquad\text{y}\qquad
\tau^2_n = \frac{1}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2_0}}\,.
$$
esto es, $\theta\mid\boldsymbol{y}\sim\textsf{N}(\mu_n,\tau_n^2)$.

5. Considere una única observación de la distribución muestral $x\mid\theta\sim\textsf{N}(\theta,\theta)$, con $\theta > 0$. Muestre que la previa de Jeffreys de para $\theta$ es tal que:
$$
p_J(\theta)\propto\frac{(2\theta + 1)^{1/2}}{\theta}\,.
$$

En este caso la distribución muestral es 
$$
p(x\mid\theta) = (2\pi\theta)^{-1/2}\,\textsf{exp}\left\{ -\frac{1}{2\theta}(x-\theta)^2 \right\}
$$
que en escala log se convierte en
$$
\log p(x\mid\theta) = -\frac12\log(2\pi\theta)-\frac{1}{2\theta}(x-\theta)^2
$$
cuya primera y segunda derivada respecto a $\theta$ son respectivamente
$$
\frac{\textsf{d}}{\textsf{d}\theta}\log p(x\mid\theta) = -\frac12\,\frac{1}{\theta} - \frac12\,\frac{x^2-\theta^2}{\theta^2}
\qquad\text{y}\qquad
\frac{\textsf{d}^2}{\textsf{d}\theta^2}\log p(x\mid\theta) = \frac12\,\frac{1}{\theta^2} - \frac12\,\frac{x^2}{\theta^3}\,.
$$
Así, la información esperada de Fisher es 
$$
\begin{align*}
I(\theta) &= -\textsf{E}_{x\mid\theta}\left( \frac{\text{d}^2}{\text{d}\theta^2}\log p(x\mid\theta) \right)\\
&= -\textsf{E}_{x\mid\theta}\left( \frac12\,\frac{1}{\theta^2} - \frac12\,\frac{x^2}{\theta^3} \right)\\
&= -\left( \frac12\,\frac{1}{\theta^2} - \frac12\,\frac{\textsf{E}_{x\mid\theta}(x^2)}{\theta^3} \right)\\
&= -\frac12\,\frac{1}{\theta^2} + \frac12\,\frac{\theta+\theta^2}{\theta^3}\\
&= \frac{2\theta+1}{\theta^2}\,.
\end{align*}
$$
En consecuencia, la distribución previa de Jeffreys es 
$$
p_J(\theta)\propto\sqrt{ \frac{2\theta+1}{\theta^2} } = \frac{(2\theta+1)^{1/2}}{\theta} 
$$
dado que $\theta > 0$.

6. Una distribución de la familia exponencial (de un parámetro) es cualquier distribución cuya función de densidad se puede expresar como
$$
p(y\mid\phi) = h(y)\,c(\phi)\,\exp{\left( \phi\,t(y) \right)}\,,
$$ 
donde $\phi$ es el parámetro natural y $t(y)$ es un estadístico suficiente. Muestre que las distribuciones Binomial, Poisson, y Exponencial hacen parte de la familia exponencial. En cada caso identificar el parámetro natural y un estadístico suficiente.

Distribución Binomial:
$$
p(y\mid n,\theta) = \binom{n}{y}\theta^y(1-\theta)^{n-y} = \binom{n}{y}(1-\theta)^n \textsf{exp}\left\{ y\log\frac{\theta}{1-\theta} \right\}=  \binom{n}{y}\cdot\frac{1}{1+e^\phi}\cdot \textsf{exp}\left\{\phi\,y \right\}
$$
donde $\phi = \log\frac{\theta}{1-\theta}$ es el parámetro natural y $t(y) = y$ es un estadístico suficiente.

Distribución Poisson:
$$
p(y\mid\theta) = \frac{e^{-\theta}\theta^y}{y!} = \frac{1}{y!}\,e^{-\theta}\,\textsf{exp}\left\{ y\log\theta \right\} = \frac{1}{y!}\cdot\textsf{exp}\left\{-e^\phi\right\}\cdot\textsf{exp}\left\{\phi\,y\right\}
$$
donde $\phi = \log\theta$ es el parámetro natural y $t(y) = y$ es un estadístico suficiente.

Distribución Exponencial:
$$
p(y\mid\theta) = \theta e^{-\theta y} = 1\cdot\phi\cdot \textsf{exp}\left\{\phi\,(-y)\right\}
$$
donde $\phi = \theta$ es el parámetro natural y $t(y) = -y$ es un estadístico suficiente.

7. La variable aleatoria $X$ tiene distribución Galenshore con parámetros $\alpha,\beta > 0$, i.e., $X\mid\alpha,\beta\sim\textsf{Galenshore}(\alpha,\beta)$, si su función de densidad de probabilidad es
$$
p(x\mid\alpha,\beta) = 
\frac{2}{\Gamma(\alpha)}\,\beta^{2\alpha}\,x^{2\alpha-1}\,e^{-\beta^2x^2}\,,\qquad x>0\,.
$$
Para esta distribución se tiene que 
$$
\textsf{E}(X\mid\alpha,\beta) = \frac{\Gamma(\alpha+\tfrac12)}{\beta\Gamma(\alpha)} \qquad\text{y}\qquad\textsf{E}(X^2\mid\alpha,\beta) = \frac{\alpha}{\beta^2}\,.
$$ 
Asumiendo que $\alpha$ es conocido:
a. Identifique una clase de densidades previas conjugadas para $\beta$.

Veamos que $X\mid\alpha,\beta\sim\textsf{Galenshore}(\alpha,\beta)$, con $\alpha$ conocido, pertenece a la familia exponencial (ver Ejercicio 6.). Dado que
$$
p(x\mid\alpha,\beta) =\frac{2}{\Gamma(\alpha)}\,\beta^{2\alpha}\,x^{2\alpha-1}\,e^{-\beta^2x^2} = 
h(x)\,c(\phi)\,\exp{\left( \phi\,t(x) \right)}\,,
$$
donde
$$
h(x) = \frac{2}{\Gamma(\alpha)}\,x^{2\alpha-1}\,, \qquad
c(\phi) = (\beta^{2})^\alpha=\phi^\alpha\,,\qquad
\exp{\left( \phi\,t(x) \right)} = \exp{\left( -\beta^2x^2 \right)}= \exp{\left( \phi\,(-x^2) \right)} \,,
$$
con $\phi = \beta^2$ y $t(x) = -x^2$. Por lo tanto, como $X\mid\alpha,\phi\sim\textsf{Galenshore}(\alpha,\phi)$, con $\alpha$ conocido, entonces las densidades de la forma
$$
p(\phi) = \kappa(n_0,t_0)\,c(\phi)^{n_0}\,\exp{\left( n_0t_0\,\phi \right)}
$$
constituyen una familia de previas conjugadas para $\phi$, donde $n_0$ y $t_0$ son hiperparámetros y $\kappa(n_0,t_0)$ es una constante de normalización (ver Hoff, 2009, Sección 3.3, pág. 51, para más detalles acerca de este resultado). Bajo esta parametrización, $n_0$ se puede interpretar como el tamaño de muestra previo y $t_0$ como una aproximación previa de del estadístico suficiente $t(x)$. Por lo tanto, sustituyendo, se tiene que las previas conjugadas para $\phi$ son tales que
$$
p(\phi) \propto \phi^{n_0\alpha}\,\exp{\left( n_0t_0\,\phi \right)}\,.
$$
Luego, usando el teorema de la transformación de variables aleatorias (e.g, https://faculty.math.illinois.edu/~r-ash/Stat/StatLec1-5.pdf) se tiene que las previas conjugadas para $\beta=\sqrt{\phi}$ son tales que
$$
p(\beta) = p_\phi(g^{-1}(\beta))\,\Big|\frac{\textsf{d}}{\textsf{d}\beta}g^{-1}(\beta)\Big| \propto \beta^{2n_0\alpha+1}\,\exp{\left( n_0t_0\,\beta^2 \right)}\,.
$$
donde $g(x)=\sqrt{x}$ y $g^{-1}(x)=x^2$. En consecuencia, la familia de previas conjuadas para $\beta$ es Galenshore con parámetros $a=n_0\alpha+1$ y $b=\sqrt{-n_0t_0}$, i.e., $\beta\sim\textsf{Galenshore}(n_0\alpha+1,\sqrt{-n_0t_0})$. Se observa que $b>0$ dado que $t_0$ corresponde a una aproximación a priori del estadístico suficiente $t(x)=-x^2$.

**Hoff, P. D. (2009). A first course in Bayesian statistical methods (Vol. 580). New York: Springer.**

b. Sea $y_1,\ldots,y_n\mid\beta\stackrel{\text{iid}}{\sim}\textsf{Galenshore}(\alpha,\beta)$. Encuentre la distribución posterior de $\beta$ dado $\boldsymbol{y}=(y_1,\ldots,y_n)$ usando la distribución previa de la clase conjugada del numeral anterior.

Si $y_i\mid\alpha,\beta \stackrel{\text{iid}}{\sim} \textsf{Galenshore}(\alpha,\beta)$, para $i = 1,\ldots,n$, con $\alpha$ conocido, y además $\beta\sim\textsf{Galenshore}(a,b)$, entonces la distribución posterior de $\beta$ está dada por
$$
\begin{align*}
p(\beta\mid\boldsymbol{y}) &\propto p(\boldsymbol{y}\mid\beta)\,p(\beta) \\
&=\prod_{i=1}^n \frac{2}{\Gamma(\alpha)}\,\beta^{2\alpha}\,y_i^{2\alpha-1}\,e^{-\beta^2y_i^2} \times \frac{2}{\Gamma(a)}\,b^{2a}\,\beta^{2a-1}\,e^{-b^2\beta^2}\\
&\propto \beta^{2n\alpha}\,\textsf{exp}\left\{ -\beta^2\sum_{i=1}^n y_i^2 \right\} \times \beta^{2a-1}\,\textsf{exp}\left\{ -b^2\beta^2 \right\}\\
&= \beta^{2(a+n\alpha)-1}\,\textsf{exp}\left\{ -\left(b^2+\sum_{i=1}^n y_i^2\right)\beta^2 \right\}
\end{align*}
$$
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$. Por lo tanto, la distribución posterior de $\beta$ es Galenshore con parámetros $a_n=a+n\alpha=(n_0+n)\alpha+1$ y $b=\sqrt{b^2+\sum_{i=1}^n y_i^2}=\sqrt{-n_0t_0+\sum_{i=1}^n y_i^2}$, i.e., $\beta\mid\boldsymbol{y}\sim\textsf{Galenshore}((n_0+n)\alpha+1,\sqrt{-n_0t_0+\sum_{i=1}^n y_i^2})$

c. Identifique un estadístico suficiente para $\beta$ a partir de la distribución condicional conjunta $p(\boldsymbol{y}\mid\beta)$.

Dado que la distribución muestral de $\boldsymbol{y}$ se puede escribir como
$$
p(\boldsymbol{y}\mid\beta) = \left(\frac{2}{\Gamma(\alpha)}\right)^n\,\left(\prod_{i=1}^n y_i \right)^{2\alpha-1}\,(\beta^2)^{n\alpha}\,\textsf{exp}\left\{ -\beta^2\sum_{i=1}^n y_i^2 \right\}=h(\boldsymbol{y})\,g(\beta,t(\boldsymbol{y}))
$$
donde
$$
h(\boldsymbol{y}) = \left(\frac{2}{\Gamma(\alpha)}\right)^n\,\left(\prod_{i=1}^n y_i \right)^{2\alpha-1}
\qquad\text{y}\qquad
g(\beta,t(\boldsymbol{y})) = (\beta^2)^{n\alpha}\,\textsf{exp}\left\{ \beta^2\left(-\sum_{i=1}^n y_i^2\right) \right\}
$$
con $t(\boldsymbol{y})=-\sum_{i=1}^n y_i^2$. Así, en virtud del teorema de factorización de Fisher-Neyman (e.g., https://online.stat.psu.edu/stat415/lesson/24/24.2), se tiene que $t(\boldsymbol{y})=-\sum_{i=1}^n y_i^2$ es un estadístico suficiente para $\beta$.

d. Determine $\textsf{E}(\beta\mid\boldsymbol{y})$.

En este caso la media posterior de $\beta$ es 
$$
\textsf{E}(\beta\mid\boldsymbol{y}) = \frac{\Gamma(a_n+1/2)}{b_n\,\Gamma(a_n)} = \frac{\Gamma(a+n\alpha+1/2)}{\sqrt{b^2+\sum_{i=1}^n y_i^2} \,\Gamma(a+n\alpha) }
$$
dado que $\beta\mid\boldsymbol{y}\sim\textsf{Galenshore}(a_n,b_n)$.

8. Suponga que $y_1\ldots,y_5$ son observaciones condicionalmente independientes de una distribución Cauchy con parámetro de localización $\theta$ y parámetro de escala 1, i.e.,
$$
p(y_i\mid\theta) = \frac{1}{\pi(1+(y_i-\theta)^2)}\,,\qquad-\infty<y_i<\infty\,,\qquad-\infty<\theta<\infty\,,
$$
para $i=1,\ldots,5$. Además, asuma por simplicidad que la distribución previa de $\theta$ es Uniforme en el intervalo $(0,100)$, i.e., $\theta\sim\textsf{Unif}(0,100)$. 

Teniendo en cuenta el vector de observaciones $\boldsymbol{y}=(43.0, 44.0, 45.0, 46.5, 47.5)$:

a. Calcule la función de densidad posterior sin normalizar, $p(\boldsymbol{y}\mid\theta)\,p(\theta)$, en una grilla de puntos equidistantes para $\theta$ de la forma $0,\frac{1}{M},\frac{2}{M},\ldots,100$, con $M=1,000$. Usando los valores calculados para cada punto de la grilla, calcule y grafique la función de densidad posterior normalizada, $p(\theta\mid\boldsymbol{y})$.

```{r}
# datos
y <- c(43.0, 44.0, 45.0, 46.5, 47.5)
# ditr. posterior en escala log sin normalizar
logp <- function(theta, y) -sum(log(1 + (y - theta)^2))
# grilla de valores para theta
M <- 1000
grilla <- seq(from = 0, to = 100, by = 1/M)
grilla <- grilla[-c(1, 100*M+1)] # remover extremos de la grilla
# calculo de la log-posterior (sin normalizar) en cada punto de la grilla
LP <- NULL
for (i in 1:length(grilla)) LP[i] <- logp(theta = grilla[i], y)
```

La constante de normalización es 
$$
k = \left( \int_\Theta p(\boldsymbol{y} \mid \theta)\, p(\theta)\, \textsf{d}\theta \right)^{-1}\approx 95.59451
$$
la cual se puede aproximar mediante $k \approx 1/(\frac{1}{M}\cdot\sum_{k=1}^M d_k) = M/\sum_{k=1}^M d_k$, donde 
$$
d_k = \textsf{exp}(\log p(\boldsymbol{y}\mid\theta_k) + \log p(\theta_k) ) = \textsf{exp}\left( -\sum_{i=1}^n \log\left(1 + (y_i-\theta_k)^2\right) \right)\,,\qquad k=1,\ldots,M\,,
$$
dado que la integral se puede aproximar por medio de la suma de rectángulos con base $1/M$ y altura $d_i$, donde $1/M$ es la amplitud que hay entre cada uno de los puntos de la grilla.

```{r}
# constante de normalizacion en escala log
logk <- log(M) - log(sum(exp(LP)))
logk
# constante de normalizacion en escala real
exp(logk)
```
Dado que este procedimiento es una aproximación discreta (basado en una grilla de valores) de la distribución posterior, en términos de recursos computacionales, solo tiene sentido práctico hacerla en bajas dimensiones (a lo más dos o tres). 

```{r}
# cálculo de la posterior en escala log normalizada
LP <- LP + logk
# cálculo de la posterior en escala real normalizada
# no es necesario dividir entre sum(exp(LP)) porque ya se normalizo en el paso anterior
P <- exp(LP)
# graficos (escala log y real)
par(mfrow=c(2,2), mar = c(3,3,1.5,1), mgp = c(1.75,.75,0))
plot(x = grilla, y = LP, type = "l", ylab = "Log-posterior", xlab = expression(theta), col = 4, lwd = 1, cex.axis = 0.8)
plot(x = grilla, y = P,  type = "l", ylab = "Posterior",     xlab = expression(theta), col = 4, lwd = 1, cex.axis = 0.8)
plot(x = grilla, y = LP, type = "l", ylab = "Log-posterior", xlab = expression(theta), col = 4, lwd = 1, cex.axis = 0.8, xlim = c(42,48))
plot(x = grilla, y = P,  type = "l", ylab = "Posterior",     xlab = expression(theta), col = 4, lwd = 1, cex.axis = 0.8, xlim = c(42,48))
```

b. Usando la aproximación discreta del numeral anterior, obtenga $B=10,000$ muestras de la distribución posterior de $\theta$ y grafique el histograma correspondiente (junto con una estimación puntual y un intervalo de credibilidad al 95\%).

Se observa que la aproximación discreta del numeral anterior coincide con la aproximación de la distribución posterior a partir de la muestras usando simulación.

```{r, eval = T, echo = F}
# numero de muestras de monte carlo
B <- 10000
# muestras de la posterior de theta
set.seed(1234)
theta_samples = sample(x = grilla, size = B, replace = T, prob = P)
# grafico
par(mfrow = c(1,1), mar = c(3,3,1.5,1), mgp = c(1.75,.75,0))
# histograma
hist(theta_samples, probability = T, xlim = c(40,50), main = "", ylab = "Posterior", xlab = expression(theta), border = "gray95", col = "gray95")
# estimacion kernel
lines(density(theta_samples), col = 2, lwd = 2)
# aproximacion
lines(x = grilla, y = P, type = "l", col = 4, lwd = 1)
# estimación puntual
abline(v = mean(theta_samples), col = 1, lty = 2)
# intervalo de credibilidad
abline(v = quantile(theta_samples, probs = c(0.025, 0.975)), col = 1, lty = 4)
legend("topleft", legend = c("Aprox. discreta","Aprox. MC","Media","Intervalo al 95%"), col = c(2,4,1,1), lty = c(1,1,2,4), bty = "n")
```
```{r, eval = F, echo = T}
# numero de muestras de monte carlo
B <- 10000
# muestras de la posterior de theta
set.seed(1234)
theta_samples = sample(x = grilla, size = B, replace = T, prob = P)
# grafico
par(mfrow = c(1,1), mar = c(3,3,1.5,1), mgp = c(1.75,.75,0))
# histograma
hist(theta_samples, probability = T, xlim = c(40,50), main = "", ylab = "Posterior", xlab = expression(theta), border = "gray95", col = "gray95")
# aproximacion MC
lines(density(theta_samples), col = 2, lwd = 2)
# aproximacion discreta
lines(x = grilla, y = P, type = "l", col = 4, lwd = 1)
# estimación puntual
abline(v = mean(theta_samples), col = 1, lty = 2)
# intervalo de credibilidad
abline(v = quantile(theta_samples, probs = c(0.025, 0.975)), col = 1, lty = 4)
legend("topleft", legend = c("Aprox. discreta","Aprox. MC","Media","Intervalo al 95%"), col = c(2,4,1,1), lty = c(1,1,2,4), bty = "n")
```

c. Utilice las muestras de la distribución posterior de $\theta$ del numeral anterior para obtener muestras de la distribución predictiva posterior de una observación futura y grafique el histograma correspondiente (junto con una estimación puntual y un intervalo de credibilidad al 95\%).

```{r, eval=TRUE, echo=F}
# distribucion predictiva
set.seed(1234)
predictiva <- rcauchy(n = B, location = theta_samples, scale = 1)
# grafico
par(mfrow=c(1,1), mar = c(3,3,1.5,1), mgp = c(1.75,.75,0))
hist(x = predictiva, freq = F, xlim = c(30, 60), nclass = 2000, main = "", ylab = "Densidad predictiva posterior", xlab = expression(y), border = "gray95", col = "gray95")
lines(density(predictiva, adjust = 2, n = 5000), col = 2, lwd = 1)
abline(v = mean(predictiva), col = 1, lty = 2)
abline(v = quantile(x = predictiva, probs = c(0.025, 0.975)), col = 1, lty = 4)
legend(x = 32, y = 0.2, legend = c("Posterior","Media","Intervalo al 95%"), col = c(2,1,1), lty = c(1,2,4), bty = "n")
```

```{r, eval=F, echo=T}
# distribucion predictiva
set.seed(1234)
predictiva <- rcauchy(n = B, location = theta_samples, scale = 1)
# grafico
par(mfrow=c(1,1), mar = c(3,3,1.5,1), mgp = c(1.75,.75,0))
hist(x = predictiva, freq = F, xlim = c(30, 60), nclass = 2000, main = "", ylab = "Densidad predictiva posterior", xlab = expression(y), border = "gray95", col = "gray95")
lines(density(predictiva, adjust = 2, n = 5000), col = 2, lwd = 1)
abline(v = mean(predictiva), col = 1, lty = 2)
abline(v = quantile(x = predictiva, probs = c(0.025, 0.975)), col = 1, lty = 4)
legend(x = 32, y = 0.2, legend = c("Posterior","Media","Intervalo al 95%"), col = c(2,1,1), lty = c(1,2,4), bty = "n")
```
